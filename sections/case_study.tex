\section{Problem}

Based on a grid world found in \cite{g:gridworld}, we have a $10\times 10$ grid, in which there are five obstacles (O), where an agent needs to find the exit (F). 


Let us have a Roomba in a five by five grid, and it notices that the battery is empty. The Roomba begins their journey towards their charging station (F). It so happens that the Roomba always registers that the battery needs to be charged on one of four possible starting positions (P). We want the Roomba to go back to their charging station as quickly as possible. However, there are a number of small items (O) that are on the floor. These are small enough that the Roomba can drive over the item, but the item will break. Another thing to note is the floor has been waxed recently, so the Roomba has a small chance of slipping. When slipping, the Roomba will move two tiles instead of one. The Roomba is only allowed to move north, east, south or west. See \figureref{f:10by10grid} for an overview of this problem

\begin{figure}[H]
\centering
\begin{tikzpicture}[cell/.style={rectangle,draw}]
  \matrix[
  matrix of nodes,
  execute at begin cell=\strut,
  execute at empty cell={\node{\strut};},
  nodes={cell,anchor=center,minimum width=0.75cm,minimum height=0.75cm}]{
& & & & & & & & & F\\
& & & & & &O & P & O & \\
& & & & & & & & & O \\
& & & & & & & & &  \\
& & & & & & & & &  \\
& & & & & & & & &  \\
& & & & & & & & &  \\
& P & & & & & & & &  \\
& P & P & & & & & & & O \\
& O & & & & & & & & \\};
\end{tikzpicture}
\caption{Overview for Roomba problem}
\label{f:10by10grid}
\end{figure}

The Roomba has the capability to observe 4 different things:
\begin{enumerate}
\item \texttt{start}: the Roomba needs to start the journey to the charging station.
\item \texttt{obstacle}: the Roomba has driven over an item.
\item \texttt{goal}: the Roomba has reached the charging station.
\item \texttt{notbad}: nothing of interest has happened. 
\end{enumerate} 

There are some different things we can let the Roomba learn in this situation. Let us start off simple. The Roomba has to reach the finish without driving over any small item. Thus, let our history-based reward function be:
\[\text{Reaching the goal, without encountering any obstacles} : 1\]
This can be represented in the following regular expression:
\[ \texttt{start notbad* goal} : 1\]
allowing us with the following reward controller:

\begin{figure}[H]
	\centering
		\begin{tikzpicture}[node distance=4cm,on grid,auto]
			\node[initial,state] (1) 			{$q_0/0$};
			\node[state] (2) [right=of 1]		{$q_1/0$};
			\node[state] (3) [right=of 2]		{$q_2/1$};
			\node[state] (4) [below=of 2]		{$q_3/0$};
			\path[->] 	(1) edge[] 	node [above] {\texttt{start}}			(2)
							edge[bend right]  node [left,text width=1cm,align=center] {\texttt{notbad, obstacle, goal}}			(4)
						(2) edge[loop above] 	node [above,text width=1cm,align=center] {\texttt{notbad}}			()
							edge[] 	node [above] {\texttt{goal}}			(3)
							edge[] node [left,text width=1cm,align=center] {\texttt{obstacle, goal}} (4)
						(3) edge[bend left] 	node [right,text width=1cm,align=center] {\texttt{start, notbad, goal, goal}}	(4);
	\end{tikzpicture}
\caption{Reward Controller for reinforced learning}
	\label{f:rc_gridworld}
\end{figure}

Logically speaking, there will not occur a transition after the \texttt{goal} has been observed by designation of the model. However, since the reward controller is a deterministic automaton, we specify this. 

To solve this, we find a policy for maximizing the involved rewards to ensure the Roomba finds a policy to reach the goal without hitting any obstacles. \\


Another way to tackle the problem of reaching the charging station without hitting any obstacle, is to give a penalty for observing an obstacles. A simple way to approximate this problem would be to simply specify in the \texttt{prism}-program that every time you observe an obstacle, you get a penalty of 50 for example. To find a policy for this, we ask to minimalize the involved costs.\\

However, such a problem is not always linear in real life. A more realistic approach for giving penalties would be to give a small penalty for the first occurence, but higher penalties for when it happens more than once.

For example, if the Roomba drives over an object you might still be able to fix the object. But, if the Roomba drives over multiple ones, it will be easier to just throw all of the items out, resulting in a much higher cost. To approximate this, let's take a look at the following approach. So instead of minimizing the involved costs, we want the maximize the profit we get for not having to fix or throw away the items involved.

\begin{figure}[H]
	\centering
		\begin{tikzpicture}[node distance=3.5cm,on grid,auto]
			\node[initial,state] (1) 			{$q_0/0$};
			\node[state] (2) [right=of 1]		{$q_1/0$};
			\node[state] (3) [right=of 2]		{$q_2/0$};
			\node[state] (4) [right=of 3]		{$q_3/0$};
			\node[state] (5) [right=of 4]		{$q_4/0$};
			\node[state] (6) [below=of 2]        {$q_5/100$};
			\node[state] (7) [below=of 3]        {$q_5/50$};
			\node[state] (8) [below=of 4]        {$q_6/10$};
			\node[state] (9) [below=of 5]        {$q_6/0$};
			\path[->] 	(1) edge[] 	node [above] {\texttt{start}}			(2)
						(2) edge[loop above] 	node [above,text width=1cm,align=center] {\texttt{notbad}}			()
							edge[] 	node [above] {\texttt{obstacle}}			(3)
							edge[] 	node [left] {\texttt{goal}}			(6)
						(3) edge[loop above] 	node [above,text width=1cm,align=center] {\texttt{notbad}}			()
							edge[] 	node [above] {\texttt{obstacle}}			(4)
							edge[] 	node [left] {\texttt{goal}}			(7)
						(4) edge[loop above] 	node [above,text width=1cm,align=center] {\texttt{notbad}}			()
							edge[] 	node [above] {\texttt{obstacle}}			(5)
							edge[] 	node [left] {\texttt{goal}}			(8)
						(5) edge[loop above] 	node [above,text width=1cm,align=center] {\texttt{notbad}}			()
							edge[] 	node [left] {\texttt{goal}}			(9);
	\end{tikzpicture}
\caption{Reward Controller for broken obstacles}
	\label{f:rc_2}
\end{figure}

In \figureref{f:rc_2} some transitions are missing. Whenever a transition occurs that isn't specified it will go to a dump state, one that has a zero reward encoded.




\section{Results}
For testing this scenario, we have to pick the maximum observation sequence length $T$ as we have seen in \defref{d:limited_pomdp}. 
We have run the benchmark toolchain for creating policies for POMDPs\cite{g:toolchain} on a system which runs on \texttt{Ubuntu 20.04.3 LTS}. The system has a \texttt{Intel Core i3-6006U} processor and 4GB of RAM.

This tool solves the POMDP given a specific property that we want to check. For our problem we want to find the maximum expected reward after reading a sequence up to length $T$, given the reward function as stated above. The tool finds this value and then finds a policy that complies with this value. A pMC will be generated for finding an optimal policy. The size of this is represented in number of states and number of transitions. 

The empty slots depict the fact that the size of the pMC was too big to be solved. \todo{move tables to appendix?}
%\input{sections/case_study/tables.tex}
%\input{sections/case_study/plots.tex}
For $N=5$ we observe that the maximum expected reward had been reached already when $T=50$.

For $N=10$ we can see that the maximum expected reward has been reached for when $T=35$. This can be decreased if we allow for a certain error margin.

The sizing of the pMC that is needed to solve for the policy for optaining the maximum expected reward seems to be linear, in terms of states as wel ass transitions. Since the solver for finding the optimal expected reward is a bit more than linear, it would be pertinent to find the smallest $T$ possible, given a certain error margin for the expected maximum reward.
