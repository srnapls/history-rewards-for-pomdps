\section{Definition}


\towrite{introduction}
The idea is to transform the history-based reward function into something more tangible. We transform it is so that we can obtain the reward per step instead of only at the end of a sequence.


Based on the history-based reward function $R:\Omega^*\to\mathbb{R}$ of a POMDP $\mathcal{M}$, we build a reward controller that mimics its behavior.

%todo: basically the same as a moore machine

%idea: when the word is done reading, you then obtain the reward. Normally when using a Moore machine you get the reward every time you enter it. However, since we won't be using the reward controller as a usual moore machine, it's fine to encode it in the state. We will see in the next chapter how we then obtain the reward.

\begin{definition}
	A reward controller $\mathcal{F}$ is a reward machine $(N,n_I, \Omega, \mathbb{R}, \delta, \lambda)$, where
	\begin{itemize}
		\item $N$, the finite set of memory nodes;
		\item $n_I\in N$, the initial memory node;
		\item $\Omega$, the input alphabet;
		\item $\mathbb{R}$, the output alphabet;
		\item $\delta: N \times \Omega \to N$, the memory update;
		\item $\sigma: N \to \mathbb{R}$, the reward output. 
	\end{itemize}
\end{definition}

\towrite{define $\delta^*$ just like DFA}


\input{sections/reward-controller/sequences.tex}

\input{sections/reward-controller/regular-expressions.tex}

