\section{Partially Observable MDPs}
Partially Observable Markov Decision Processes have been used to model problems
concerning sequential decision-making where the model is uncertain. The objective is
to act in a way that optimizes a performance criterion. This often translates itself to
maximizing a certain reward, or minimizing certain costs involved.

\subsection{Definitions}

\begin{definition}[Partially Observable Markov Decision Process]
	A Partially Observable Markov Decision Process, or POMDP, is a tuple $(S, \mathcal{A}, \Omega, T, O, r, b_0)$, where
	\begin{itemize}
		\item $S$, the state space;
		\item $A$, the action space;
		\item $\Omega$, the observation space;
		\item $T:S\times \mathcal{A} \times S \to [0,1]$, the transition function where
			$T(s,a,s')=Pr(s'\mid s,a)$;
		\item $O:S\times \mathcal{A} \times \Omega \to [0,1]$, the observation function where
		$O(s',a,o)= Pr(o\mid s',a)$
		\item $r: S \times \mathcal{A} \to \mathds{R}$, the reward function, and 
		\item $b_0 : S \to [0,1]$, the initial probability distribution over states.
	\end{itemize}
\end{definition}

A POMDP is only partially observable, which indicates that we do not know everything about the model. We specifically are unaware of what state a POMDP currently is in. To create a notion of what state we believe to be in, we use belief states.

\begin{definition}[Belief state]
	Given a POMDP $(S, \mathcal{A}, \Omega, T, O, r, b_0)$, a belief state $b$ is a probability distribution over $S$ called $\Delta$. The probability of the POMDP being in state $s$ according to belief $b$, is denoted by $b(s)$. Logically $\sum_{s\in S}b(s)=1$ and for all states $s$ we have that $b(s)\in[0,1]$.
\end{definition}

A belief state evolves over time and a POMDP will start in their initial belief state $b_0$.
The belief state will be updates according to the action that is undertaken and an observation that will be observed after taking said action.

\begin{definition}[Updated belief state]
Let $Pr(o|a,b)=\sum_{s,s'\in S}O(s'',a,o)T(s,a,s'')b(s)$. The probability of ending in state $s'$ after performing an action $a$ and then observing observation $o$ is calculated as
\begin{equation*}
	b^{a,o}(s')=\frac{O(s,a,o)}{Pr(o | a,b)}\sum_{s\in S} T(s,a,s')b(s)
\end{equation*}
\end{definition}

With the help of belief states, we can create a fully observable continuous space belief MDP representing a POMDP.

\begin{definition}[Belief MDP]
	A POMDP $(S, A, \Omega, T, O, r, b_0)$ can be written as a belief MDP $(\Delta, \mathcal{A}, \tau, \rho)$, where
	\begin{itemize}
		\item $\Delta = \Pi(S)$, the set of probability distributions over S;
		\item $\mathcal{A}$, the action space;
		\item $\tau : \Delta \times \mathcal{A} \times \Delta \to [0,1]$, the new transition function
		\item $\rho: \Delta \times \mathcal{A} \to \mathds{R}$, the new reward function.
	\end{itemize}
\end{definition}

The objective is to maximize the expected cumulative reward, which is done by looking for an optimal policy. Let $\gamma$ be some discount factor and $\rho_t$ be the expected immediate reward obtained at a certain step $t$, then the expected cumulative reward is defined as

\begin{equation*}
	J^\pi(b)=E \big[\sum_{t=0}^{\infty}\gamma \rho_t\mid b,\pi\big]
\end{equation*}

With this information, we can calculate the optimal policy 
\begin{equation}
\pi^* = \argmax_{\pi\in \mathcal{A}^\Delta} J^\pi(b_0)
\end{equation}

The value $J^{\pi^*}(b_0)$ can be computed recursively through a value function $V_n$, due to Bellman's principle of optimality\cite{p:bellman}. If we let $H$ be the possible infinite horizon of the problem, the optimal value can be concluded from $J^{\pi^*}(b)=V_{n=H}(b)$.

\begin{definition}[Value function for POMDPs]
For $b\in\Delta$, let the value function be defined as
\begin{flalign}
	V_0(b) & = 0 \nonumber \\
	V_n(b) & = \max_{a\in \mathcal{A}}\big[\rho(b,a) + \gamma\int_{\Delta}\tau(b,a,b')V_{n-1}(b')db'\big] \nonumber \\
	\label{eqn:value_fun}
	&= \max_{a\in \mathcal{A}} \big[\rho(b,a) + \gamma\sum_{o\in\Omega} Pr(o\mid a, b) V_{n-1}(b^{a,o}) \big] 
\end{flalign}
\end{definition}

Observe that the reward function for a POMDP is based on a state and action through $r(s,a)$, while the reward function for a belief MDP depends on the belief state and action through $\rho(b,a)$. The belief MDP reward function $\rho$ can be derived from the POMDP reward function $r$ like so
\begin{equation}
	\label{eqn:reward_belief_mdp}
	\rho(b,a)=\sum_{s\in S}b(s)r(s,a)
\end{equation}

When the $\rho$ is defined as in Equation~\ref{eqn:reward_belief_mdp}, the recursive computation in Equation~\ref{eqn:value_fun} has the property to generate piecewise-linear and convex (PWLC) value functions for each horizon\cite{p:pwlc}.  

\begin{definition}[Piecewise-linear function]
	A function is piecewise-linear if it consists of $n$ linear segments defined over $n$ intervals.
\end{definition}

\begin{definition}[Convex function]
	A function $f:A\to B$ is convex if for all $\theta\in[0,1]$ and $x_1,x_2\in A$
	\begin{equation*}
		f(\theta x_1 + (1-\theta) x_2)\leq \theta f(x_1) + (1-\theta)f(x_2)
	\end{equation*}
\end{definition}

For example, if we claim that $\Gamma_n$ is the set of vectors representing the value function for horizon $n$, we can rewrite the value function to $V_n(b)=\max_{\alpha\in\Gamma_n}\sum_{s\in S} b(s)\alpha(s)$. 

\subsection{Solving with exact updates}
Using the PWLC property, Equation~\ref{eqn:value_fun} can be refactored to be able to perform the Bellman update. Let $\chi_n(b,s)=(\chi_n(b))(s)$ where $\chi_n(b)=\argmax_{\alpha\in \Gamma_n} b\cdot\alpha$, so $\chi_n(b)$ returns the hyperplane that is maximal for a given belief state $b$. The refactored value function is then written as 

\begin{equation}
	V_n(b) = \max_{a\in \mathcal{A}}\sum_{o\in \Omega}\sum_{s\in S}b(s)\big[\frac{r(s,a)}{|\Omega|} + \sum_{s'\in S}T(s,a,s')O(s',a,o)\chi_{n-1}(b^{a,o},s')\big]
\end{equation}

Let’s solely look at the terms within in the brackets. They are dependent on a certain action and observation. So for a given $o\in\Omega, a\in\mathcal{A}$, the set of $\alpha$-vectors is generated by $\overline{\Gamma_n}^{a,o}$. Let $r^a(s)=r(s,a)$ and $P^{a,o}(s,s')=T(s,a,s')O(s',a,o)$ in 
\begin{equation}
	\overline{\Gamma_n}^{a,o}=\{ \frac{r^a}{|\Omega|}+P^{a,o}\cdot\alpha_{n-1}\mid\alpha_{n-1}\in\Gamma_{n-1} \}
\end{equation}

Observe that there are $|\mathcal{A}|\times|\Omega|$ of such $\overline{\Gamma_n}^{a,o}$ sets. To create the set containing all
of these possible observation and action combination we take the following cross-sum:
\begin{equation}
	\overline{\Gamma_n} = \bigcup_{a\in\mathcal{A}}\bigoplus_{o\in \Omega}\overline{\Gamma_n}^{a,o}
\end{equation}
Note that the operator $\oplus$ is defined as $U \oplus W = \{u + w | u \in U, w \in W\}$.\\

Since $\overline{\Gamma_n}$ is created through all possible combination, it also contains $\alpha$-vectors that are useless because the corresponding hyperplanes are below the value function. To create the actual $\Gamma_n$ set from $\overline{\Gamma_n}$, we can use one of several pruning techniques on the $\overline{\Gamma_n}$ set which will get rid of the dominating useless vectors.

\section{Extension for Active Sensing}

All problems who only have partial observability tackle a certain issue through getting information about the model or achieving some goal. Most of the time, the concerning problem is tackled through obtaining enough information get an optimal reward. However, some \textit{active sensing} problems (where the objective is acting in a way to acquire knowledge about a part of the model or minimizing the entropy over a given state
variable) cannot be resolved this way. For these problems, the performance of an agent should incorporate their obtained knowledge of the system thus far. \\

In Equation~\ref{eqn:reward_belief_mdp}, we observe that the reward of a POMDP is solely based on the current state and action. We want the agent to include their obtained knowledge, but simply putting the entire history of the model in the state, would lead to a combinatorial explosion. \\

To tackle this issue, we propose a new method of defining rewards based on the acquired knowledge through the usage of belief states. Since we will use a different definition than the one described in Equation~\ref{eqn:reward_belief_mdp}, the direct link with POMDPs is broken. Luckily, a lot of the components of a POMDP are still usable. For this we use a generalization of the POMDP framework, where the reward is directly defined as a function $\rho(b,a)$.

\subsection{Definitions}
\begin{definition}[$\rho$POMDP]
	A $\rho-$based POMDP ($\rho$POMDP) is a tuple $(S,\mathcal{A},T,O,\rho,b_0)$, where
		\begin{itemize}
		\item $S$, the state space;
		\item $A$, the action space;
		\item $\Omega$, the observation space;
		\item $T:S\times \mathcal{A} \times S \to [0,1]$, the transition function where
		$T(s,a,s')=Pr(s'\mid s,a)$;
		\item $O:S\times \mathcal{A} \times \Omega \to [0,1]$, the observation function where
		$O(s',a,o)= Pr(o\mid s',a)$;
		\item $\rho: \Delta \times \mathcal{A} \to \mathds{R}$, the reward function, and ;
		\item $b_0 : S \to [0,1]$, the initial probability distribution over states.
	\end{itemize}
\end{definition}

We see that $\rho$ is often just some uncertainty or error measure, but it can also be combined with some expected state-reward.

\subsection{Convexity property}

Observe that the belief-based value function for POMDPs is convex (Equation~\ref{eqn:value_fun}), because $r(s,a)$ is linear in Equation~\ref{eqn:reward_belief_mdp}. The value function for $\rho$-POMDPs is also convex, if $/rho(b,a)$ is convex.

\begin{theorem}
	If $\rho$ and $V_0$ are convex functions over $\delta$, then the value function $V_n$ of the belief MDP is convex over $\Delta$ at any time step $n$.
\end{theorem}

Note that $\rho(b,a)$ being convex over $\Delta$ is due to the fact that uncertainty measures are naturally convex\cite{p:belief-dependent-rewards}. We will only be focusing on reward functions that are convex. The initial value function $V_0=0$ for finite-horizon problems, but can be any convex function for infinite-horizon problems.

\subsection{Solving with exact updates}
We assume that this belief-based $\rho$ is a PWLC function. Therefore it can be represented as several $\Gamma$-sets; one $\Gamma^a_\rho$ for each action in $\mathcal{A}$:
\begin{equation}
	\label{eqn:reward_belief}
	\rho(b,a)=\max_{\alpha\in\Gamma^a_\rho}\big[\sum_{s}b(s)\alpha(s)\big]
\end{equation}

We now rewrite the value function from Equation~\ref{eqn:value_fun} together with the new reward
function from Equation~\ref{eqn:reward_belief}. Let $\chi_\rho^a(b,s) = (\chi_\rho^a(b))(s)$ where $\chi_\rho^a(b)=\argmax_{\alpha\in\Gamma^a_\rho}(b\cdot\alpha)$. The rewritten value function for $\rho$POMDPs is then
\begin{equation}
	V_n(b)= \max_{a\in \mathcal{A}}\sum_{s\in S}b(s) \big[\chi_\rho^a(b,s) + \sum_{o\in \Omega}\sum_{s'\in S}T(s,a,s')O(s',a,o)\chi_{n-1}(b^{a,o},s') \big]
\end{equation}

This new value function makes use of $\Gamma_\rho^a$ through $\chi_\rho^a$, but also generates $|\mathcal{A}|\times|\Omega|$ $\Gamma$-sets called $\overline{\Gamma_n}^{a,o}$, where $P^{a,o}(s,s')=T(s,a,s')O(s',a,o)$, defined as:
\begin{equation}
	\overline{\Gamma_n}^{a,o}=\{P^{a,o}\cdot\alpha_{n-1}\mid\alpha_{n-1}\in\Gamma_{n-1}\}
\end{equation}

Just like for POMDPs, we can use exact algorithms for $\rho$POMDPs. The difference is that now the cross-sum to calculate $\overline{\Gamma_n}$ now includes 
\begin{itemize}
	\item one $\alpha^{a,o}$ for each observation $\Gamma$-set $\overline{\Gamma_n}^{a,o}$, and
	\item one $\alpha_\rho$ from the $\Gamma$-set $\Gamma_\rho^a$ corresponding to the reward.
\end{itemize}
This results in the new cross-sum 
\begin{equation}
	\overline{\Gamma_n}=\bigcup_{a\in\mathcal{A}}\big[\bigoplus_{o\in \Omega}\overline{\Gamma_n}^{a,o}\oplus\Gamma_\rho^a\big]
\end{equation}

The number of vectors that are being generated is larger than for a regular POMDP. Let $|R|$ be the number of $\alpha$-vectors specifying the $\rho(b,a)$ function, which depends on the action that was taken. The total amount of vectors is thus $|R| \times |\mathcal{A}| × |\Omega|$.