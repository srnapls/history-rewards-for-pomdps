\begin{definition}
	A POMDP is a tuple $(S, \mathcal{A}, \Omega, T, O, r, b_0)$, where
	\begin{itemize}
		\item $S$, the state space;
		\item $A$, the action space;
		\item $\Omega$, the observation space;
		\item $T:S\times \mathcal{A} \times S \to [0,1]$, the transition function where
			$T(s,a,s')=Pr(s'\mid s,a)$;
		\item $O:S\times \mathcal{A} \times \Omega \to [0,1]$, the observation function where
		$O(s',a,o)= Pr(o\mid s',a)$
		\item $r: S \times \mathcal{A} \to \mathds{R}$, the reward function, and 
		\item $b_0 : S \to [0,1]$, the initial probability distribution over states.
	\end{itemize}
\end{definition}


\towrite{how/why belief mdp are nice}
\towrite{belief states and how they're updated}

A belief state $b\in\Delta=\Pi(S)$.

\begin{definition}
Let $b^{a,o}$ be the probability distribution after performing an action $a$ and then observing observation $o$. The probability of ending in state $s'$ after performing $a$ and observing $o$ then can be calculated as follows, where $Pr(o|a,b)=\sum_{s,s'\in S}O(s'',a,o)T(s,a,s'')b(s)$,
\begin{equation}
	b^{a,o}(s')=\frac{O(s,a,o)}{Pr(o | a,b)}\sum_{s\in S} T(s,a,s')b(s)
\end{equation}
\end{definition}

\begin{definition}
	A POMDP $(S, A, \Omega, T, O, r, b_0)$ can be written as a belief MDP $(\Delta, \mathcal{A}, \tau, \rho)$, where
	\begin{itemize}
		\item $\Delta = \Pi(S)$, the set of probability distributions over S;
		\item $\mathcal{A}$, the action space;
		\item $\tau : \Delta \times \mathcal{A} \times \Delta \to [0,1]$, the new transition function
		\item $\rho: \Delta \times \mathcal{A} \to \mathds{R}$, the new reward function.
	\end{itemize}
\end{definition}

A belief MDP is continuous, so 
\towrite{what does continutuity have to do w/ it?}

\towrite{why wanna have optimal expected reward}

The objective is to maximize the expected cumulative reward, which is done by looking for an optimal policy. Let $\gamma$ be some discount factor and $\rho_t$ be the expected immediate reward obtained at a certain step $t$, then the expected cumulative reward is defined as

\begin{equation}
	J^\pi(b)=E \big[\sum_{t=0}^{\infty}\gamma \rho_t\mid b,\pi\big]
\end{equation}

Then the optimal policy can be calculated as
\begin{equation}
\pi^* = \argmax_{\pi\in \mathcal{A}^\Delta} J^\pi(b_0)
\end{equation}

\begin{definition}
For $b\in\Delta$, let the value function be defined as
\begin{flalign}
	V_0(b) & = 0 \nonumber \\
	V_n(b) & = \max_{a\in \mathcal{A}}\big[\rho(b,a) + \gamma\int_{\Delta}\tau(b,a,b')V_{n-1}(b')db'\big] \nonumber \\
	\label{eqn:value_fun}
	&= \max_{a\in \mathcal{A}} \big[\rho(b,a) + \gamma\sum_{o\in\Omega} Pr(o\mid a, b) V_{n-1}(b^{a,o}) \big] 
\end{flalign}
\end{definition}

The function $J^{\pi^*}$ can be computed recursively through this value function, due to Bellman's principle of optimality\cite{p:bellman}. Let $H$ be the possible infinite horizon of the problem, then$J^{\pi^*}(b)=V_{n=H}(b)$.

Observe that the reward function for a POMDP is based on a state and action through $r(s,a)$, while the reward function for a belief MDP depends on the a belief state and action through $\rho(b,a)$. The belief MDP reward function $\rho$ can be derived from the POMDP reward function $r$ :
\begin{equation}
	\label{eqn:reward_belief_mdp}
	\rho(b,a)=\sum_{s\in S}b(s)r(s,a)
\end{equation}

When the $\rho$ is defined as in Equation~\ref{eqn:reward_belief_mdp}, the recursive computation in Equation~\ref{eqn:value_fun} has the property to generate piecewise-linear and convex (PWLC) value functions for each horizon\cite{p:pwlc}.  

\begin{definition}
	A function is piecewise-linear if it consists of $n$ linear segments defined over $n$ intervals.
\end{definition}

\begin{definition}
	A function $f:A\to B$ is convex if for all $\theta\in[0,1]$ and $x_1,x_2\in A$
	\begin{equation*}
		f(\theta x_1 + (1-\theta) x_2)\leq \theta f(x_1) + (1-\theta)f(x_2)
	\end{equation*}
\end{definition}

Let $\Gamma_n$ be the set of vectors representing the value function for horizon $n$, then the value function can be rewritten to $V_n(b)=\max_{\alpha\in\Gamma_n}\sum_{s\in S} b(s)\alpha(s)$. 

Using the PWLC property, Equation~\ref{eqn:value_fun} can be refactored to be able to perform the Bellman update. Let $\chi_n(b)=\arg\max_{\alpha\in \Gamma_n} b\dot\alpha$ and $\chi_n(b,s)=(\chi_n(b))(s)$ in 

\begin{equation}
	V_n(b) = \max_{a\in \mathcal{A}}\sum_{o\in O}\sum_{s\in S}b(s)\big[\frac{r(s,a)}{|\Omega|} + \sum_{s'\in S}T(s,a,s')O(s',a,o)\chi_{n-1}(b^{a,o},s')\big]
\end{equation}

\begin{equation}
	\overline{\Gamma_n}^{a,o}=\{ \frac{r^a}{|\Omega|}+P^{a,o}\cdot\alpha_{n-1}\mid\alpha_{n-1}\in\Gamma_{n-1} \}
\end{equation}


Problem:
a special kind of problem is when the performance criterion incorporates an explicit measure of the agent's knowledge about the system, which is based on the beliefs rather than states.

the reward of a pomdp cannot model this since it's only based on the current state and action.
we prefer to consider a new way of defining rewards based on the acquired knowledge represented by belief states.

the direct link with pomdps is broken, but we can fix this by generalizing the pomdp framework to a $rho$-baes POMDP $\rho$POMDP, where the reward is not defined as a function $r(s,a)$ but directly as a function $\rho(b,a)$

$\rho(b,a)$ is not restricted to be only an uncertainty measurement, but can be a combination of the expected state-action rewards and an uncertainty or error measurement.



A belief-based value function is convex. 

If $\rho$ and $V_0$ are convex functions over $\Delta=\Pi(s)$, then the value function $V_n$ of the belief MDP is convex over $\Delta$ at any time step $n$.

Last theorem is based on $\rho(b,a)$ being a convex function over $b$, which is a natural property. A reward function meant to reduce the uncertainty mush provide high payloads near the corners of the simplex and low payloads near its center. Which is why we're only focusing on reward functions that comply with convexity. $V_0$ might be any convex function for infinite-horizon problems, but $V_0=0$ for finite-horizon problems.


From now on $\rho$ is a PWLC function, and can thus be represented as several $\Gamma$-sets, one $\Gamma^a_\rho$ for each $a$:
\begin{equation}
	\rho(b,a)=\max_{\alpha\in\Gamma^a_\rho}\big[\sum_{s}b(s)\alpha(s)\big]
\end{equation}

Changes into a new value function
\begin{equation}
	V_n(b)= \max_{a\in A}\sum_{s}b(s) \big[\argmax_{\alpha\in\Gamma^a} \rho(b\cdot\alpha) + \sum_{o}\sum_{s'}T(s.a.s')O(s',a,o)\chi_{n-1}(b^{a,o},s') \big]
\end{equation}


\begin{equation}
	\overline{\Gamma_n}^{a,o}=\{P^{a,o}\cdot\alpha_{n-1}\mid\alpha_{n-1}\in\Gamma_{n-1}\}
\end{equation}
