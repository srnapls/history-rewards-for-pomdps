\section{Definition}

The idea is that we have some sort of history-based reward function $R:\Omega^*\to\mathbb{R}$ which belongs to some POMDP $\mathcal{M}$. Based on the reward function alone, we are going to build a machine that controls the reward associated to its sequence. 

Since a sequence of obersations is nothing more than a word in $\Omega^*$ we are going to build a finite automaton over the alphabet $\Omega$. Then when we have read any word $\pi\in\Omega^*$, we want that the state we end up in to contain the reward associated with $\pi$. This is in some sense the same as a Moore machine, except for the fact that instead of applying $\sigma$ to every state we encouter, we only use $\sigma$ on the last state obtained. 

\begin{definition}
	A reward controller $\mathcal{F}$ is a Moore machine $(N,n_I, \Omega, \mathbb{R}, \delta, \sigma)$, where
	\begin{itemize}
		\item $N$, the finite set of memory nodes;
		\item $n_I\in N$, the initial memory node;
		\item $\Omega$, the input alphabet;
		\item $\mathbb{R}$, the output alphabet;
		\item $\delta: N \times \Omega \to N$, the memory update;
		\item $\sigma: N \to \mathbb{R}$, the reward output. 
	\end{itemize}
\end{definition}

Note that when we discuss $|\mathcal{F}|$, we are discussing the number of states in the reward controller $\mathcal{F}$, so $|\mathcal{F}|=|N|$. When reading a sequence of observations, or a word in $\Omega^*$, we wish to know in what memory node we end up in because we are interested in the reward encoded into that state. Which is why we we use the following definition, similarly as what we have defined for DFAs.

\begin{definition}
We define $\delta^*:N\times\Omega^*\to N$ where $\delta^*(n,w)$ denotes the state we end up after reading word \texttt{seq} starting from state $n$ as follows
\begin{equation*}
\delta^*(n,\texttt{seq})=\begin{cases}
	n &\text{if } \texttt{seq}=\lambda \\
	\delta^*(\delta(n,o_1),o_2\dots o_n) & \text{if } \texttt{seq}=o_1 o_2\dots o_n
	\end{cases}
\end{equation*}
\label{d:delta_star_rc}
\end{definition}

\towrite{since we let the base cost be zero, note that we cannot allow for minimizing policies. convert cost to profit, given a start-base.}

\subsection*{Implementation}
For the implementation of a Reward Controller, we have used the DFA construction as described in \cite{g:moore}. This DFA construction allows us to map a value for every state. For simple DFAs this would be a \texttt{0} for non-acceptance, and a \texttt{1} for acceptance. In our case, we can simply encode the appropriate reward in their states. 