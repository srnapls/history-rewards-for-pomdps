\section{Definition}

The idea is that we have some sort of history-based reward function $R:\Omega^*\to\mathbb{R}$ which belongs to some POMDP $\mathcal{M}$. Based on the reward function alone, we are going to build a machine that controls the reward associated to its sequence. 

Since a sequence of obersations is nothing more than a word in $\Omega^*$ we are going to build a finite automaton over the alphabet $\Omega$. Then when we have read any word $\pi\in\Omega^*$, we want that the state we end up in to contain the reward associated with $\pi$. This is in some sense the same as a Moore machine, except for the fact that instead of applying $\sigma$ to every state we encouter, we only use $\sigma$ on the last state obtained. 

\begin{definition}
	A reward controller $\mathcal{F}$ is a Moore machine $(N,n_I, \Omega, \mathbb{R}, \delta, \lambda)$, where
	\begin{itemize}
		\item $N$, the finite set of memory nodes;
		\item $n_I\in N$, the initial memory node;
		\item $\Omega$, the input alphabet;
		\item $\mathbb{R}$, the output alphabet;
		\item $\delta: N \times \Omega \to N$, the memory update;
		\item $\sigma: N \to \mathbb{R}$, the reward output. 
	\end{itemize}
\end{definition}

When reading a sequence of observations, or a word in $\Omega^*$, we wish to know in what memory node we end up in because we are interested in the reward encoded into that state. Which is why we we use the following definition, similarly as what we have defined for DFAs.

\begin{definition}
We define $\delta^*:N\times\Omega^*\to N$ where $\delta^*(n,w)$ denotes the state we end up after reading word $\pi$ starting from state $n$ as follows
\begin{equation*}
\delta^*(n,\pi)=\begin{cases}
	n &\text{if } \pi=\epsilon \\
	\delta^*(\delta(n,o_1),o_2\dots o_n) & \text{if } \pi=o_1 o_2\dots o_n
	\end{cases}
\end{equation*}
\label{d:delta_star_rc}
\end{definition}
