\section{From regular expressions}
\label{sec:rc-regex}
Given a number of regular expressions over observations defined as $e_1,e_2,\dots,e_n$ together with their respective rewards $r_1,r_2,\dots,r_n\in\mathbb{R}$. Let us define a reward function $R$ that maps the regular expression to their respective reward, in other words $R(e_i)=r_i$. \\

We want to create a reward controller that mimics the behaviour of several regular expressions and their associated rewards. Note that we only want a reward when the sequence of observations is accepted by the language generated by the regular expression. The first step would be is to create a DFA that is generated by the regular expression given. This can be done through simply turning the regular expression into a Non-Deterministic Finite Automaton (with $\epsilon$-transitions) and then turning that into a DFA or using other known methods\cite{p:regex-to-dfa}. All that is left for a single regular expression is to keep track of the rewards associated to their final states.\\

So given the $n$ regular expression, we create $n$ DFAs. Let $D_i=(Q_i,q_{0,i},\Omega, \delta_i,F_i)$ be the DFA that accepts the language generated by $e_i$. And then per construction we have that $L(D_i)=L(e_i)$. \\

Note that since we want to obtain a reward controller, we have to encode the reward in the nodes. This is solved by only encoding the reward of DFA $D_i$ in all states of $F_i$. For example if $pi\in\Omega^*$ gets accepted by $D_i$, we have to make sure that the state it ends up in - i.e. the final state(s) - has the reward encoded in its state(s). This is done by the following definition.
\begin{definition}
Let $R_A:Q_1\cup Q_2\cup\dots\cup Q_n\to\mathbb{R}$ be a function that maps any state $q$ of all the state spaces of $D_1,D_2,\dots,D_n$ to their respective rewards. If $q$ is a final state of DFA $D_i$ it should get the reward corresponding to the regular expression used for that specific DFA. In other words,
\begin{equation*}
R_A(q) = \begin{cases}
R(e_i) & \text{ if } q\in F_i \\
0 & \text{ otherwise }
\end{cases}
\end{equation*}
\label{d:associated_reward}
\end{definition}

Having obtained all these seperate DFAs, we can now create a DFA that will accept any word that is accepted by any of the seperate DFAs as follows.

\begin{definition}
The induced product DFA for given DFAs $D_1,D_2,\dots,D_n$ where $D_i=(Q_i,q_{0,i},\Sigma,\delta_i,F_i)$ is a tuple $D=(Q,q_0,\Sigma,\delta,F)$ where 
\begin{itemize}
\item $Q = Q_1 \times Q_2 \times \dots \times Q_n$
\item $q_0 = \langle q_{0,1}, q_{0,2}, \dots, q_{0,n}\rangle$
\item $\Omega$, the same input alphabet
\item $\delta(\langle q_1,q_2,\dots,q_n\rangle,a)= \langle \delta_1(q_1,a), \delta_2(q_2,a),\dots,\delta_n(q_n,a)\rangle$
\item $F=\{\langle q_1,q_2,\dots,q_n\rangle \mid \exists i \in \{1,2,\dots,n\} : q_i\in F_i\}$
\end{itemize}
\label{d:product_automaton}
\end{definition}

\begin{lemma}
Given $n$ DFAs where $D_i=(Q_i,q_{0,i}, \Sigma,\delta_i,F_i)$, let $D$ be the product automaton as obtained in \defref{d:product_automaton}. Then we $L(D)=L(D_1)\cup L(D_2)\cup \dots L(D_n)$.
\begin{proof}
	\begin{flalign*}
		w\in L(D) &\Longleftrightarrow \delta_N^*(q_0,w)\in F \\
		& \Longleftrightarrow \langle \delta_1^*(q_{0,1},w), \delta_2^*(q_{0,2},w),\dots,\delta_n^*(q_{0,n},w)\rangle\in F \\
		&\Longleftrightarrow \exists i\in\{1,\dots,n\} :  \delta_i^*(q_{0,i},w)\in F_i \\
		&\Longleftrightarrow \delta_1^*(q_{0,1},w)\in F_1 \text{ or } \delta_2^*(q_{0,2},w)\in F_2 \text{ or } \dots \text{ or } \delta_n^*(q_{0,n},w)\in F_n \\
		& \Longleftrightarrow w \in L(D_1) \text{ or } w\in L(D_2) \text{ or } \dots \text{ or } w\in L(D_n)\\
		&\Longleftrightarrow w\in L(D_1)\cup L(D_2)\cup \dots \cup L(D_n)
	\end{flalign*}
\end{proof}
\end{lemma}

The only step left to obtain the reward controller is to connect the obtained product DFA together with the associated rewards of the states.

\begin{definition}
Given a (product) DFA $N=(Q,q_0,\Omega,\delta,F)$ and the associated reward function $R_A$, we define the induced reward controller $\mathcal{F}=(N,n_I,\Omega, \mathbb{R}, \delta_\mathcal{F},\sigma)$ as follows
\begin{itemize}
\item $N=Q$
\item $n_I=q_0$
\item $\delta_\mathcal{F}=\delta$
\item $\sigma: Q\to\mathbb{R}$ where $\sigma(\langle q_1,q_2,\dots, q_n\rangle) = \sum\limits _{i=1}^n R_A(q_i)$
\end{itemize}
\label{d:reward_controller_regex}
\end{definition}

Note that the $\sigma$ is defined by taking the sum over the associated rewards. This is because if we have a sequence $\pi\in\Omega^*$ that is accepted by several regular expressions given, it should then obtain all the seperate rewards associated with those regular expressions. Through the following lemma we ensure that for any sequence $\pi\in\Omega^*$ the reward controller obtains the combination of rewards depending on the final state after having read $\pi$.

\begin{lemma}
Given $e_1,e_2,\dots,e_n$ a sequence of regular expression together with their associated rewards $r_1,r_2,\dots,r_n$, let $D$ be the product automaton as defined in \defref{d:product_automaton} build from the DFAs $D_i$ for which $L(D_i)=L(e_i)$. Then let $\mathcal{F}=(N,n_I,\Omega,\mathbb{R},\delta,\sigma)$ be the reward controller as defined in \defref{d:reward_controller_regex} given $D$. We say that for all possible words $\pi\in\Omega^*$ the following holds:
\[\sigma(\delta^*(n_I,\pi))=\sum\limits_{e\in\{e_i\mid\pi\in L(e)\}}R(e_i)\]
\begin{proof}
\begin{flalign}
\sigma(\delta^*(n_I,\pi)) &= \sigma(\langle q1,q2,\dots,q_n\rangle)  \label{p:r_l1} \\
	&= \sum\limits_i^n R_A(q_i)\label{p:r_l2}\\
	&= \sum\limits_{\substack{i\in\{1,\dots,n\}\\ q_i\in F_i}} R_A(q_i) \label{p:r_l3}\\
&= \sum\limits_{\substack{i\in\{1,\dots,n\}\\ q_i\in F_i}} R(e_i) \label{p:r_l4}\\
&= \sum\limits_{\substack{i\in\{1,\dots,n\}\\ \delta^*(q_{0,i},\pi) \in F_i }} R(e_i)\label{p:r_l5}\\
&= \sum\limits_{\substack{i\in\{1,\dots,n\}\\ \pi\in L(D_i)}} R(e_i)\label{p:r_l6}\\
&= \sum\limits_{\substack{i\in\{1,\dots,n\}\\ \pi\in L(e_i)}}R(e_i)\label{p:r_l7}\\
&= \sum\limits_{e\in\{e_i\mid\pi\in L(e_i)\}} R(e)\label{p:r_l8}
\end{flalign}
For \equref{p:r_l1} we simply use \defref{d:delta_star_rc} and the fact that $D$ is deterministic, so it ends up in an unique state after reading $\pi$.
For \equref{p:r_l2} we use the definition for $\sigma$ as seen in \defref{d:reward_controller_regex}. For \equref{p:r_l3} we use that fact that in \defref{d:associated_reward} we observe that $R_A(q_i)$ is equal to zero if $q_i\notin F_i$ and only produces a non-zero value for all $q_i\in F_i$. Thus we only look at the $q_i$ which return a non-zero value. Since we now know we only look at the non-zero reward values, we can use \defref{d:associated_reward} again in \equref{p:r_l4}. From \defref{d:delta_star} we can rewrite the equation in \equref{p:r_l5}. For \equref{p:r_l6} we use \defref{d:accepted_language}. Since per construction $L(e_i)=L(D_i)$ for all $i\in\{1,\dots,n\}$, we rewrite the term in \equref{p:r_l7}. Finally in \equref{p:r_l8} we simply rewrite the term under the sum.
\end{proof}
\end{lemma}


\subsection*{Example}
Let's say we are given 2 regular expressions. One is that an even number off $\square$ gives a reward of $10$ and the other states that an uneven number of $\blacksquare$ gives a reward of $15$. In other words 
$R(e_1)=R(\texttt{even number of }\square)=10$ and $R(e_2)=R(\texttt{uneven number of }\blacksquare)=15$\\

Let us first obtain the two DFAs that are generated by $e_1$ and $e_2$. Those can be seen in \figureref{f:e1_e2}. 
\begin{figure}[H]

\begin{subfigure}[H]{0.4\textwidth}
	\centering
		\begin{tikzpicture}[node distance=3cm,on grid,auto]
			\node[initial,state,accepting] (1) 				{$q_{0,1}$};
			\node[state] (2) [right=of 1]		{$q_{1,1}$};
			\path[->] 	(1) edge[bend left] 	node [above] {$\square$}			(2)
							edge[loop above] node [above] {$\blacksquare$}   ()
						(2) edge[bend left] 	node [above] {$\square$}			(1)
							edge[loop above] node [above] {$\blacksquare$}   ();
	\end{tikzpicture}
\caption{DFA for regular expression even number of $\square$}
\end{subfigure}
\hfill
\begin{subfigure}[H]{0.4\textwidth}
	\centering
		\begin{tikzpicture}[node distance=3cm,on grid,auto]
			\node[initial,state] (1) 				{$q_{0,2}$};
			\node[state,accepting] (2) [right=of 1]		{$q_{1,2}$};
			\path[->] 	(1) edge[bend left] 	node [above] {$\blacksquare$}			(2)
							edge[loop above] node [above] {$\square$}   ()
						(2) edge[bend left] 	node [above] {$\blacksquare$}			(1)
							edge[loop above] node [above] {$\square$}   ();
	\end{tikzpicture}
\caption{DFA for regular expression for odd number of $\blacksquare$}
\end{subfigure}
\caption{}
\label{f:e1_e2}
\end{figure}
Then we create the product automaton as defined in \defref{d:product_automaton}. The result can be seen in \figureref{f:pd}. 

\begin{figure}[H]
	\centering
		\begin{tikzpicture}[node distance=3cm,on grid,auto]
			\node[initial,state,accepting] (1) 				{$\langle q_{0,1}, q_{0,2} \rangle$};
			\node[state] (2) [right=of 1]		{$\langle q_{1,1}, q_{0,2} \rangle$};
			\node[state,accepting] (3) [below=of 1]		{$\langle q_{0,1}, q_{1,2} \rangle$};
			\node[state,accepting] (4) [right=of 3]		{$\langle q_{1,1}, q_{1,2} \rangle$};
			\path[->] 	(1) edge[bend left] 	node [above] {$\square$}			(2)
							edge[bend left] 	node [left] {$\blacksquare$}			(3)
						(2) edge[bend left] 	node [above] {$\square$}			(1)
							edge[bend left] 	node [left] {$\blacksquare$}			(4)
						(3) edge[bend left] 	node [left] {$\blacksquare$}			(1)
							edge[bend left] 	node [above] {$\square$}			(4)
						(4) edge[bend left] 	node [left] {$\blacksquare$}			(2)
							edge[bend left] 	node [above] {$\square$}			(3);
	\end{tikzpicture}
\caption{Product DFA for both regular expressions}
	\label{f:pd}
\end{figure}

From this we then obtain the reward controller as per \defref{d:reward_controller_regex}, and can be found in \figureref{f:rc}. Note that 
\begin{flalign*}
R_A(q_{0,1})&=10\\
R_A(q_{1,1})&= R_A(q_{0,2})=0\\
R_A(q_{1,2})&=15
\end{flalign*}

\begin{figure}[H]
	\centering
		\begin{tikzpicture}[node distance=4cm,on grid,auto]
			\node[initial,state] (1) 			{$\langle q_{0,1}, q_{0,2} \rangle /10$};
			\node[state] (2) [right=of 1]		{$\langle q_{1,1}, q_{0,2} \rangle /0$};
			\node[state] (3) [below=of 1]		{$\langle q_{0,1}, q_{1,2} \rangle /25$};
			\node[state] (4) [right=of 3]		{$\langle q_{1,1}, q_{1,2} \rangle /15$};
			\path[->] 	(1) edge[bend left] 	node [above] {$\square$}			(2)
							edge[bend left] 	node [left] {$\blacksquare$}			(3)
						(2) edge[bend left] 	node [above] {$\square$}			(1)
							edge[bend left] 	node [left] {$\blacksquare$}			(4)
						(3) edge[bend left] 	node [left] {$\blacksquare$}			(1)
							edge[bend left] 	node [above] {$\square$}			(4)
						(4) edge[bend left] 	node [left] {$\blacksquare$}			(2)
							edge[bend left] 	node [above] {$\square$}			(3);
	\end{tikzpicture}
\caption{Reward Controller for $R$}
	\label{f:rc}
\end{figure}