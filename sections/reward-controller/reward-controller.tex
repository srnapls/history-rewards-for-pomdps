\section{Definition}


\towrite{introduction}
The idea is to transform the history-based reward function into something more tangible. We transform it is so that we can obtain the reward per step instead of only at the end of a sequence.


Based on the history-based reward function $R:\Omega^*\to\mathbb{R}$ of a POMDP $\mathcal{M}$, we build a reward controller that mimics its behavior.

%todo: basically the same as a moore machine

%idea: when the word is done reading, you then obtain the reward. Normally when using a Moore machine you get the reward every time you enter it. However, since we won't be using the reward controller as a usual moore machine, it's fine to encode it in the state. We will see in the next chapter how we then obtain the reward.

\begin{definition}
	A reward controller $\mathcal{F}$ is a reward machine $(N,n_I, \Omega, \mathbb{R}, \delta, \lambda)$, where
	\begin{itemize}
		\item $N$, the finite set of memory nodes;
		\item $n_I\in N$, the initial memory node;
		\item $\Omega$, the input alphabet;
		\item $\mathbb{R}$, the output alphabet;
		\item $\delta: N \times \Omega \to N$, the memory update;
		\item $\sigma: N \to \mathbb{R}$, the reward output. 
	\end{itemize}
\end{definition}


\section{Creating a reward controller from a list of sequences}
\towrite{it's a choice that you only get one reward after an input. if a seperate reward sequence is a strict substring of another sequence, they are not added. this can be done by adding the reward to every following node in the created sequence.!!!!!}
Let's say we are designing a model for an engineer and they want certain observation sequences to connect to a reward. Thus we are given a number of observation sequences $\pi_1,\pi_2,\dots,\pi_n$ together with their associated real valued rewards $r_1,r_2,\dots,r_n$. 
\begin{definition}
Given the observation sequences $\pi_1,\pi_2,\dots,\pi_n$ and their associated rewards $r_1,r_2,\dots,r_n$ we define the history-based reward function $R:\Omega^*\to\mathbb{R}$, which we create as follows
\[R(w) = \begin{cases}
	r_i &\text{ if } w=\pi_i \text { for } i\in \{1,\dots,n\} \\
	0   &\text{ otherwise}
	\end{cases}\]
	\label{d:created_reward_function}
\end{definition}
In $R$ we simply connect the observation sequence $\pi_i$ to their respective reward $r_i$ and every other sequence is connected to zero.\\

We only want to obtain any of the rewards if their associated observation sequence has been observed. Thus we create a reward controller in which we encode the reward in the state. The idea is as follows: if we read the observation sequence and we end up in a certain state $n$, we obtain the reward $\sigma(n)$ in that state. 

Given all the sequences over which the Non-Markovian reward function is defined, let us create a reward controller through the following procedure. Note that we assume that all the sequences are unique.
\begin{algorithm}[H]
	\begin{algorithmic}[1]
		\Procedure{CreateRewardController}{sequences, $R$}
		\Require sequences
		\Require $R : \Omega^* \to \mathbb{R}$
		\State $n_I \gets $ \code{new Node()} \Comment{initial node} \label{l:initial_node}
		\State $n_F \gets $ \code{new Node()} \Comment{dump node}  \label{l:dump_node}
		\State \texttt{path}$(n_I)=\lambda$ \label{l:empty_path}
		\State $N\gets\{n_I,n_F\}$
		\ForAll{$\pi=o_1 o_2\dots o_k$ in sequences}
			\State $n \gets n_I$
			\For{$i\gets 1,\dots, k$}
				\If{$\delta(n,o_i)$ is undefined}
					\State $n'\gets$ \code{new Node()} \Comment{create new memory node} \label{l:new_transition}
					\State \texttt{path}$(n)=o_1\dots o_i$
					\State $N \gets N \cup \{n'\}$		
					\State $\delta(n,o_i) \gets n'$	\label{l:set_transition}
				\EndIf 
				\State $n \gets \delta(n,o_i)$     \Comment{update memory node}
			\EndFor
			\State $\sigma(n) \gets R(\pi)$ \Comment{set reward} \label{l:set_reward}
		\EndFor
		\ForAll{$n\in N$} \Comment{makes $\delta$ and $\sigma$ deterministic}
			\ForAll{$o\in\Omega$}
				\If{$\delta(n,o)$ is undefined} \Comment{useless transition}
					\State $\delta(n,o) \gets n_F$ \label{l:self_loop}
				\EndIf
			\EndFor
			\If{$\sigma(n)$ is undefined}
				\State $\sigma(n) \gets 0$ \label{l:set_zero}
			\EndIf
		\EndFor
		\State \Return $(N,n_I,\Omega,\mathbb{R},\delta,\sigma)$
		\EndProcedure
	\end{algorithmic}
	\caption{Procedure for turning a list of sequences into a reward controller}
	\label{procedure:into_reward_controller}
\end{algorithm}

We start by creating an initial node in \lineref{l:initial_node} and a dump node in \lineref{l:dump_node}. The idea is that, since the reward controller is deterministic, if we need to determine the reward of a sequence that is (for example) longer than a known sequence (with reward), we don't want to end in the state in which the reward is encoded. Thus these zero-reward sequences are passed along to a node which will only consist of self-loops and will have a reward of zero encoded to them.

Then for every sequence which we are given, we walk through it. If we then come across a transition which isn't defined yet, we define it by making a new memory node in \lineref{l:new_transition}, adding it to $N$, and setting the transition to this new node. If the transition already existed, we simply update the memory node. After we are done with reading the sequence, we simply encode the reward into the state itself in \lineref{l:set_reward}.

Then since the reward controller needs to be deterministic, we set the other undefined values. Every other transition that hasn't been made yet, will be transferred to the dump node as mentioned above in \lineref{l:self_loop}. Furthermore, there are still nodes in which the reward is undefined. None of the given sequences ended up in these states, so per \defref{d:created_reward_function} we encode those to zero in \lineref{l:set_zero}.

We observe that the number of memory nodes $|N|$ of the newly created reward controller $\mathcal{F}$ is bounded by $\Omega^k$, where $k=\max_{seq\in sequences} |seq|$.\\

Note that the set of nodes $N$ without $n_F$ together with the memory update function is represents a directed acyclic graph. This indicates for every node $n$ there is an unique path from the initial node $n_I$ to node $n$. This unique path is encoded in the function \texttt{path}$:N\setminus n_F\to\Omega^*$. This function is well-defined, since it's defined for $n_I$ in \lineref{l:empty_path}. Every other time a new node is neccesary, it is created in \lineref{l:new_transition}, and \texttt{path} is then immediately defined for the new node. This \texttt{path} function is needed for proving the following lemma. 


\begin{lemma}
\todo{let maarten read this}
For any sequence $\pi\in\Omega^*$, let $r=R(\pi)$ be its associated reward. Then $\sigma(\delta^*(n_I,\pi))=r$.
\begin{proof}
Let us state that after reading $\pi$, we end up in state $n$, so $n = \delta^*(n_I,\pi)$. \\
Now if $n=n_F$, we know that the associated reward is zero since $\sigma(n_F)=0$. A sequence can only end up in $n_F$ if it was not a part of the pre-defined sequences and following \defref{d:created_reward_function} the reward is then zero. \\
If $n\in N\setminus n_F$, we can obtain the unique path to node $n$ through $\texttt{path}(n)$. We know that this is equal to $\pi$, so the associated reward is thus $R(\texttt{path}(n))=R(\pi)=r$.
\end{proof}
\end{lemma}


\subsection*{Example}
Say we are given the following sequences and rewards
\begin{enumerate}
	\item $\square\ \square\  $ with a reward of 15
	\item $\blacksquare\ \square\ \blacksquare\ $ with a reward of 20
	\item $\square\ \square\ \blacksquare\ \square\ $ with a reward of 12
	\item  $\blacksquare\ $ with a reward of 2
\end{enumerate}

Following the procedure~\ref{procedure:into_reward_controller} we create the associated reward controller. To show how the procedure works, we will show you the intermediate reward controller after processing every sequence. 

\subsubsection*{After sequence (1)}
\begin{figure}[H]
	\centering
		\begin{tikzpicture}[node distance=3cm,on grid,auto,shorten >=1pt,thick]
			\node[initial,state] (1) 				{$n_I$};
			\node[state] (2) [above right=of 1]		{$n_1$};
			\node[state] (3) [above right=of 2]		{$n_2/15$};
			\node[state] (9) [right=of 3]            {$n_F$};
			\path[->] 	(1) edge 	node [above] {$\square$}			(2)
						(2) edge 	node [above] {$\square$}			(3);
	\end{tikzpicture}
\caption{Reward controller after sequence (1)}
\end{figure}

\subsubsection*{After sequence (2)}
\begin{figure}[H]
	\centering
		\begin{tikzpicture}[node distance=3cm,on grid,auto,shorten >=1pt]
			\node[initial,state] (0) 				{$n_I$};
			\node[state] (1) [above right=of 0]		{$n_1$};
			\node[state] (2) [above right=of 1]		{$n_2/15$};
			\node[state] (3) [below right=of 0]      {$n_3$};
			\node[state] (4) [above right=of 3]      {$n_4$};
			\node[state] (5) [below right=of 4]      {$n_5/20$};
			\node[state] (8) [right=of 5]            {$n_F$};
			\path[->] 	(0) edge 	node [above] {$\square$}			(1)
							edge    node [above] {$\blacksquare$}	(3)
						(1) edge 	node [above] {$\square$}			(2)
						(3) edge    node [above] {$\square$}        	(4)
						(4) edge		node [above] {$\blacksquare$}	(5);
	\end{tikzpicture}
\caption{Reward controller after sequence (1) and (2)}
\end{figure}

\subsubsection*{After sequence (3)}
\begin{figure}[H]
	\centering
		\begin{tikzpicture}[node distance=3cm,on grid,auto,shorten >=1pt]
			\node[initial,state] (0) 				{$n_I$};
			\node[state] (1) [above right=of 0]		{$n_1$};
			\node[state] (2) [above right=of 1]		{$n_2/15$};
			\node[state] (3) [below right=of 0]      {$n_3$};
			\node[state] (4) [above right=of 3]      {$n_4$};
			\node[state] (5) [below right=of 4]      {$n_5/20$};
			\node[state] (6) [below right=of 2]      {$n_6$};
			\node[state] (7) [above right=of 6]      {$n_7/12$};
			\node[state] (8) [right=of 7]            {$n_F$};
			\path[->] 	(0) edge 	node [above] {$\square$}			(1)
							edge    node [above] {$\blacksquare$}	(3)
						(1) edge 	node [above] {$\square$}			(2)
						(2) edge		node [above] {$\blacksquare$}	(6)
						(3) edge    node [above] {$\square$}        	(4)
						(4) edge		node [above] {$\blacksquare$}	(5)
						(6) edge		node [above] {$\square$}			(7)
						;
	\end{tikzpicture}
\caption{Reward controller after sequence (1), (2) and (3)}
\end{figure}

\subsubsection*{After sequence (4)}
\begin{figure}[H]
	\centering
		\begin{tikzpicture}[node distance=3cm,on grid,auto,shorten >=1pt]
			\node[initial,state] (0) 				{$n_I$};
			\node[state] (1) [above right=of 0]		{$n_1$};
			\node[state] (2) [above right=of 1]		{$n_2/15$};
			\node[state] (3) [below right=of 0]      {$n_3/2$};
			\node[state] (4) [above right=of 3]      {$n_4$};
			\node[state] (5) [below right=of 4]      {$n_5/20$};
			\node[state] (6) [below right=of 2]      {$n_6$};
			\node[state] (7) [above right=of 6]      {$n_7/12$};
			\node[state] (8) [right=of 7]            {$n_F$};
			\path[->] 	(0) edge 	node [above] {$\square$}			(1)
							edge    node [above] {$\blacksquare$}	(3)
						(1) edge 	node [above] {$\square$}			(2)
						(2) edge		node [above] {$\blacksquare$}	(6)
						(3) edge    node [above] {$\square$}        	(4)
						(4) edge		node [above] {$\blacksquare$}	(5)
						(6) edge		node [above] {$\square$}			(7)
						;
	\end{tikzpicture}
\caption{Reward controller after sequence (1), (2) and (3)}
\end{figure}
\towrite{Notes for the model: missing actions/states}
Following the procedure~\ref{procedure:into_reward_controller} we can create the reward controller as seen in \figureref{f:elaborated_rc}. Note that everywhere where the red lines are depicted, those paths are obsolete. And following the procedure those end-markings should be transitions towards the memory node they originated from. The green highlighted are the rewards that are set in \lineref{l:set_reward}. The elaborated version shown in the figure first checks the observation and then check the action to then direct it to a new memory node. A more compact version can be seen in \figureref{f:simple_rc}, where the observation and the action are condensed into one transition. Note that in this figure, the self-loops are also not shown. 

\begin{figure}[H]
	\centering
		\begin{tikzpicture}[node distance=3cm,on grid,auto]
			\node[initial,state] (1) 				{$n_I$};
			\node[state] (2) [above right=of 1]		{$n_2$};
			\node[state] (3) [below right=of 1]		{$n_3$};
			\node[state] (5) [above right=of 3]		{$n_5$};
			\node[state] (6) [below right=of 3]		{$n_6$};
			\node[state] (4) [above=of 5]			{$n_4$};
			\node[state] (7) [right=of 5]			{$n_7$};
			\path[->] 	(1) edge[bend left] 	node [left] {$\square,a$}			(2)
							edge[bend right] 	node [left] {$\square,b$}			(3)
						(2) edge 				node [above] {$\square,b$}			(4)
						(3) edge[bend left] 	node [left] {$\square,a$}   		(5)
							edge[bend right] 	node [left] {$\blacksquare,c$}    (6)
						(4) edge[bend left] 	node [left] {$\square,b$}			(7)
						(5) edge 				node [above] {$\blacksquare,c$}	(7)
						(6)	edge[bend right] 	node [left] {$\blacksquare,c$}	(7);
	\end{tikzpicture}
\label{f:simple_rc}
\caption{Compact reward controller}
\end{figure}

\begin{corollary}
	For every path $\pi\in\Omega^*$ that is defined for $R_k$, we have that $R_k(\pi)=\code{FinalReward}(n_I,\pi)$.
\end{corollary}

\section{Creating a Reward Controller from a sequence of regular expressions}
\todo{find ref for product construction}
Given a number of expressions over which $R$ is defined as $e_1,e_2,\dots,e_n$, which are regular expressions over observations.
For all $i=\{1,\dots, n\}$, from the regular expression $e_i$ create a DFA $M_i=(Q_i,q_{0,i},\Omega, \delta_i,F_i)$. From all these seperate DFAs, we now create the product DFA, which encapsulates the entire problem as follows. 

\begin{definition}
The induced product DFA for given DFAs $D_1,D_2,\dots,D_n$ is $N=(Q,q_0,\Sigma,\delta,F)$ where 
\begin{itemize}
\item $Q = Q_1 \times Q_2 \times \dots \times Q_n$
\item $Q = \langle q_{0,1}, q_{0,2}, \dots, q_{0,n}\rangle$
\item $\Omega$, the same input alphabet
\item $\delta(\langle q_1,q_2,\dots,q_n\rangle,a)= \langle \delta_1(q_1,a), \delta_2(q_2,a),\dots,\delta_n(q_n,a)\rangle$
\item $F=\{\langle q_1,q_2,\dots,q_n\rangle \mid \exists i \in \{1,2,\dots,n\} : q_i\in F_i\}$
\end{itemize}
\label{d:product_automaton}
\end{definition}

\begin{lemma}
Given $n$ DFAs where $D_i=(Q_i,q_{0,i}, \Sigma,\delta_i,F_i)$, let $N$ be the product automaton as obtained in \defref{d:product_automaton}. Then we $L(N)=L(D_1)\cup L(D_2)\cup \dots L(D_n)$.
\begin{proof}
	\begin{flalign*}
		w\in L(N) &\Longleftrightarrow \delta_N^*(q_0,w)\in F \\
		& \Longleftrightarrow \langle \delta_1^*(q_{0,1},w), \delta_2^*(q_{0,2},w),\dots,\delta_n^*(q_{0,n},w)\rangle\in F \\
		&\Longleftrightarrow \exists i\in\{1,\dots,n\} :  \delta_i^*(q_{0,i},w)\in F_i \\
		&\Longleftrightarrow \delta_1^*(q_{0,1},w)\in F_1 \text{ or } \delta_2^*(q_{0,2},w)\in F_2 \text{ or } \dots \text{ or } \delta_n^*(q_{0,n},w)\in F_n \\
		& \Longleftrightarrow w \in L(D_1) \text{ or } w\in L(D_2) \text{ or } \dots \text{ or } w\in L(D_n)\\
		&\Longleftrightarrow w\in L(D_1)\cup L(D_2)\cup \dots \cup L(D_n)
	\end{flalign*}
\end{proof}
\end{lemma}


\towrite{$R_A$ keeps track of all the rewards associated with their respective regular expression}
\begin{equation*}
R_A(q) = \begin{cases}
R(e_i) & \text{ if } q\in F_i \\
0 & \text{ otherwise }
\end{cases}
\end{equation*}



\begin{definition}
So having this DFA $N=(Q,q_0,\Omega,\delta,F)$ and associated reward function $R_A$, we define the induced reward controller $mathcal{F}=(N,n_I,\Omega, \mathbb{R}, \delta_\mathcal{F},\sigma)$ as follows
\begin{itemize}
\item $N=Q$
\item $n_I=q_0$
\item $\delta_\mathcal{F}=\delta$
\item $\sigma: Q\to\mathbb{R}$ where $\sigma(\langle q_1,q_2,\dots, q_n\rangle) = \sum\limits _{i=1}^n R_A(q_i)$
\end{itemize}
\end{definition}

\subsection*{Example}
Let's say an even number off $\square$ gives a reward of $10$ and an uneven number of $\blacksquare$ gives a reward of $15$. In other words \todo{are the regular expressions really neccesary?}
$R((\blacksquare^* \square \blacksquare^*\square\blacksquare^*)^*)=10$ and \\
$R(\square^*\blacksquare \square^*(\blacksquare \square^*\blacksquare \square^*)^*)=15$
\begin{figure}[H]

\begin{subfigure}[H]{0.4\textwidth}
	\centering
		\begin{tikzpicture}[node distance=3cm,on grid,auto]
			\node[initial,state,accepting] (1) 				{$q_{0,1}$};
			\node[state] (2) [right=of 1]		{$q_{1,1}$};
			\path[->] 	(1) edge[bend left] 	node [above] {$\square$}			(2)
							edge[loop above] node [above] {$\blacksquare$}   ()
						(2) edge[bend left] 	node [above] {$\square$}			(1)
							edge[loop above] node [above] {$\blacksquare$}   ();
	\end{tikzpicture}
\caption{DFA for regular expression even number of $\square$}
\end{subfigure}
\hfill
\begin{subfigure}[H]{0.4\textwidth}
	\centering
		\begin{tikzpicture}[node distance=3cm,on grid,auto]
			\node[initial,state] (1) 				{$q_{0,2}$};
			\node[state,accepting] (2) [right=of 1]		{$q_{1,2}$};
			\path[->] 	(1) edge[bend left] 	node [above] {$\blacksquare$}			(2)
							edge[loop above] node [above] {$\square$}   ()
						(2) edge[bend left] 	node [above] {$\blacksquare$}			(1)
							edge[loop above] node [above] {$\square$}   ();
	\end{tikzpicture}
\caption{DFA for regular expression for odd number of $\blacksquare$}
\end{subfigure}
\\
\begin{subfigure}[H]{0.4\textwidth}
	\centering
		\begin{tikzpicture}[node distance=3cm,on grid,auto]
			\node[initial,state,accepting] (1) 				{$\langle q_{0,1}, q_{0,2} \rangle$};
			\node[state] (2) [right=of 1]		{$\langle q_{1,1}, q_{0,2} \rangle$};
			\node[state,accepting] (3) [below=of 1]		{$\langle q_{0,1}, q_{1,2} \rangle$};
			\node[state,accepting] (4) [right=of 3]		{$\langle q_{1,1}, q_{1,2} \rangle$};
			\path[->] 	(1) edge[bend left] 	node [above] {$\square$}			(2)
							edge[bend left] 	node [left] {$\blacksquare$}			(3)
						(2) edge[bend left] 	node [above] {$\square$}			(1)
							edge[bend left] 	node [left] {$\blacksquare$}			(4)
						(3) edge[bend left] 	node [left] {$\blacksquare$}			(1)
							edge[bend left] 	node [above] {$\square$}			(4)
						(4) edge[bend left] 	node [left] {$\blacksquare$}			(2)
							edge[bend left] 	node [above] {$\square$}			(3);
	\end{tikzpicture}
\caption{Product DFA for both regular expressions}
\end{subfigure}
\hfill
\begin{subfigure}[H]{0.4\textwidth}
	\centering
		\begin{tikzpicture}[node distance=3cm,on grid,auto]
			\node[initial,state,accepting] (1) 				{$\langle q_{0,1}, q_{0,2} \rangle/25$};
			\node[state] (2) [right=of 1]		{$\langle q_{1,1}, q_{0,2} \rangle/0$};
			\node[state] (3) [below=of 1]		{$\langle q_{0,1}, q_{1,2} \rangle/10$};
			\node[state] (4) [right=of 3]		{$\langle q_{1,1}, q_{1,2} \rangle$/15};
			\path[->] 	(1) edge[bend left] 	node [above] {$\square$}			(2)
							edge[bend left] 	node [left] {$\blacksquare$}			(3)
						(2) edge[bend left] 	node [above] {$\square$}			(1)
							edge[bend left] 	node [left] {$\blacksquare$}			(4)
						(3) edge[bend left] 	node [left] {$\blacksquare$}			(1)
							edge[bend left] 	node [above] {$\square$}			(4)
						(4) edge[bend left] 	node [left] {$\blacksquare$}			(2)
							edge[bend left] 	node [above] {$\square$}			(3);
	\end{tikzpicture}
\caption{Reward Controller for $R$}
\end{subfigure}
\end{figure}
Note that \\
$R_A(q_{0,1})=10$\\
$R_A(q_{1,1})=0$\\
$R_A(q_{0,2})=0$\\
$R_A(q_{1,2})=15$