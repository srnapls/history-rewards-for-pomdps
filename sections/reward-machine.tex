\towrite{introduction}
%add extra final state, in which we encode the actual transitions, the end state will be used to marked you are done. 

\begin{definition}
	\label{def:reward-mdp}
	The induced POMDP for reward controller $\mathcal{F}=(N, n_I, \Omega, \mathcal{R}, \delta, \lambda)$ on a POMDP $\mathcal{M}=(M,\Omega,Obs)$ where $M=(S,s_I,A,T_{M})$ is a tuple $\mathcal{M'}=(M',\Omega',Obs')$ where 
	\begin{itemize}
		\item $M' = (S',s'_I,A',T_{M'},\mathcal{R})$, the hidden MDP defined as follows
		\begin{itemize}
			\item $S'=S\times N \cup \{s_F\}$
			\item $s'_I = \langle s_I, \delta(n_I, O(s_I))\rangle$
			\item $A'=A\cup \{\texttt{end}\}$
			\item $T_{M'}:S'\times A \to \Pi(S)$ where
				\begin{align*}
					T_{M'}(\langle s,n\rangle,a,\langle s',n'\rangle) &= \begin{cases}
						T_M(s,a,s') & \text{if } \delta(n,O(s')=n') \\
						0 & \text{otherwise}
					\end{cases}\\
					T_{M'}(s,\texttt{end},s_F) &= 1 \text{ for all} s\in S'
				\end{align*}
			\item $R:S'\times A'\times S' \to \mathcal{R}$ where 
                \begin{align*}
					R(s',\texttt{end},s_F) &= 
					\begin{cases}
						\sigma(n) & \text{ if } s'=\langle s,n\rangle \\
						0         & \text{ if } s'=s_F
					\end{cases}
					\\
					R(s,a,s') &= 0 \text{ for all } s'\in S'\setminus \{s_F\}
				\end{align*}
		\end{itemize}
		\item $\Omega'=\Omega \cup \{o_F\}$, the observation state
		\item $Obs:S'\to \Omega'$ where 
			\begin{equation*}
				Obs'(s)= \begin{cases}
				Obs(s') & \text{if } s=\langle s',n\rangle\\
				o_F & \text{if } s=s_F
				\end{cases}
			\end{equation*}
		\end{itemize}
\end{definition}


