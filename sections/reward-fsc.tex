Note that $\Pi(V)$ stands for a probability distribution over $V$. So $\Pi(V)=V\to [0,1]$ where $\sum_{v\in V}v=1$.

\begin{definition}
	A labeled Markov Decision Process (labeled MDP) is a tuple $M=(S,s_I,Act,T,\mathcal{P},L,R_k)$ where 
	\begin{itemize}
		\item $S$, the finite set of states;
		\item $s_I$, the initial state;
		\item $Act$, the finite set of actions;
		\item $T:S\times A\to \Pi(S)$, the probabilistic transition function;
		\item $\mathcal{P}$, a set of propositional values;
		\item $L:S\times Act\times S\to 2^\mathcal{P}$, the labeling function;
		\item $R_k:(S\times Act)^k\times S \to \mathbb{R}$, the history-based reward function.
	\end{itemize}
\end{definition}

It's sufficient to use labels to express a reward function~\cite{p:jirp}.

\begin{definition}
	A labeled partially observable MDP (labeled POMDP) is a tuple $\mathcal{M}=(M, Z, O, R_k)$ where 
	\begin{itemize}
		\item $M=(S,s_I,Act,T,\mathcal{P},L)$, the hidden MDP;
		\item $Z$, the finite set of observations;
		\item $O:S\to Z$, the observation function;
	\end{itemize}
\end{definition}

Based on the Non-Markovian reward function $R_k$, we build a finite-state controller that mimics their behavior.

\begin{definition}
	A reward finite-state controller (R-FSC) for a reward function of a POMDP $\mathcal{M}$ is a tuple $\mathcal{F}=(N,n_I,\delta,\mathcal{R})$ where
	\begin{itemize}
		\item $N$, the finite set of memory nodes;
		\item $n_I$, the initial memory node;
		\item $\delta: N \times Z \times Act \to \Pi(N)$, the memory update;
		\item $\mathcal{R}: N \times Z \times Act\times N \to \mathbb{R}$, the reward update. 
	\end{itemize}
\end{definition}

We use the notation $\delta(n,o,a,n')$ for the probability of ending up in the memory node $n'$ starting in the memory node $n$, showing observation $o$ and performing action $a$. Thus, $\delta(n,o,a,n')=\delta(n,o,a)(n') $.\\

When working with the trajectory $s_0 a_0 s_1 a_1 \dots s_{k+1}\in(S\times Act)^k\times S$ - which is a realization of the stochastic process performed a POMDP $\mathcal{M}$ - we also obtain two sequences from the connected R-FSC $\mathcal{F}$. First we obtain the memory sequence $n_0 n_1 \dots n_k$, where $n_0=n_I$ and $n_{i+1}$ is obtained with probability $\delta(n_i,O(n_i),a_i,n_{i+1})$. Second, we obtain the reward sequence $r_0 r_1 \dots r_k$. In this reward sequence we claim that $r_i=R(n_i,O(s_i),a_i,n_{i+1})$. \\

Since $\mathcal{F}$ simulates $R_k$ of $\mathcal{M}$, we want the reward of $R_k$ and of $\mathcal{F}$ to be the same when using the same trajectory. Thus $R_k(s_0 a_0 s_1 a_1 \dots s_{k+1})= \mathcal{R}(n_{k},O(s_{k}),a_{k},n_{k+1})=r_k$.\\

Since we have a labeling function, we can obtain a corresponding label sequence $l_0 l_1 \dots l_k\in 2^{\mathcal{P}}$ where $l_i=L(s_i,a_i,s_{i+1})$ following the same trajectory. These labels are an important part when constructing the reward machine that models the non-Markovian reward function $R_k$, since they form the input alphabet. 

\begin{definition}
	The induced Reward Machine for R-FSC on a labeled POMDP is a tuple $\mathcal{A}=(V,v_I,2^{\mathcal{P}},\mathbb{R},\hat{\delta},\sigma)$ where 
	\begin{itemize}
		\item $V=S\times N$, the finite set of states;
		\item $v_I=(s_I,n_I)$, the initial state;
		\item $2^{\mathcal{P}}$, the input alphabet;
		\item $\mathbb{R}$, the output alphabet;
		\item $\hat{\delta}:V\times 2^{\mathcal{P}} \to V$, the transition function. We state $\hat{\delta}((s,n),l)=(s',n')$ if there are $a\in Act,s'\in S,n'\in N$ such that $L(s,a,s')=l$ and $\delta(n,O(s),a,n')>0$. \todo{what if multiple actions allow for the specific label}
	 \item $\sigma:V\times 2^{\mathcal{P}} \to \mathbb{R}$, the output function. We state
		$\sigma((s,n),l)=\mathcal{R}(n,O(s),a,n')$ if there are $a\in Act,n'\in N$ such that $L(s,a,s')=l$ and $\delta(n,O(s),a,n')>0$. \todo{what if multiple actions allow for the specific label}
	\end{itemize}
\end{definition}

%We say that a reward machine encodes the reward function $R_k$ of a labeled POMDP if for every trajectory and the corresponding label sequence the reward sequence equals $\mathcal{A}(l_0\dots l_k)$.