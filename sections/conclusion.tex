\towrite{conclusion and future work}

any history-based reward function can lead to extreme growth in the final induced pomdp. 

the reward controller defined in 4 currently only looks at sequence of observations, but this can also be adjusted to a sequence of actions, states, observations or a combination of those. 

\section{Future Work}
\towrite{mentioned in 4, use state of the art automata learners to decrease the size of the final pomdp}

optimizing the creation of the extended induced pomdp, a lot of states are created that are obsolete (especially if you have a placement action in your pomdp, see case study)


finding optimal $T$