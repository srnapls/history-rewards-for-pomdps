In this chapter we combine the obtained reward controller $\mathcal{F}$, which is based on the history-based reward function, together with the original POMDP. This allows us to remove the history-based aspect of the reward function, allowing us to calculate the reward step-by-step. This can be seen in \secref{s:induced_pomdp}. Firstly, since there is no method to ending the process we add another action to forcibly \textit{end} the process in \secref{s:extended_pomdp}. In \secref{s:induced_example} we will show an example. In \secref{s:induced_policy}. In \secref{s:induced_limit} we talk about on how to limit the computational process by only allowing sequences up to a certain length. In \secref{s:induced_implementation} we talk a bit about the implementation regarding this new induced POMDP.


\section{Extended POMDP}
\input{sections/induced-pomdp/extended-pomdp.tex}

\section{Induced POMDP}
\input{sections/induced-pomdp/definition.tex}

\section{Example}
\input{sections/induced-pomdp/example.tex}

\section{Optimal policy}
\input{sections/induced-pomdp/policy.tex}

\section{Limiting the observation sequence}
\input{sections/induced-pomdp/counter.tex}

\section{Implementation}
\input{sections/induced-pomdp/implementation.tex}

