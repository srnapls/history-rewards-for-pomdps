\towrite{introduction}
%add extra final state, in which we encode the actual transitions, the end state will be used to marked you are done. 

\begin{definition}
	\label{def:reward-mdp}
	The induced POMDP for reward controller $\mathcal{F}=(N, n_I, \Omega, \mathcal{R}, \delta, \lambda)$ on a POMDP $\mathcal{M}=(M,\Omega,Obs)$ where $M=(S,s_I,A,T_{M})$ is a tuple $\mathcal{M_\mathcal{F}}=(M_\mathcal{F},\Omega',Obs')$ where 
	\begin{itemize}
		\item $M_\mathcal{F} = (S',s'_I,A',T_{M_\mathcal{F}},\mathcal{R})$, the hidden MDP defined as follows
		\begin{itemize}
			\item $S'=S\times N \cup \{s_F\}$
			\item $s'_I = \langle s_I, \delta(n_I, O(s_I))\rangle$
			\item $A'=A\cup \{\texttt{end}\}$
			\item $T_{M_\mathcal{F}}:S'\times A' \to \Pi(S')$ where
				\begin{align*}				
					T_{M_\mathcal{F}}(s,\texttt{end},s_F) &= 1 \text{ for all } s\in S'\\
					T_{M_\mathcal{F}}(\langle s,n\rangle,a,\langle s',n'\rangle) &= \begin{cases}
						T_M(s,a,s') & \text{if } \delta(n,O(s')=n') \\
						0 & \text{otherwise}
					\end{cases}
				\end{align*}
			\item $R:S'\times A'\times S' \to \mathcal{R}$ where 
                \begin{equation*}
					R(s,a,s') = \begin{cases}
					\sigma(n) & \text{if } a=\texttt{end} \text{ and } s'=\langle s'',n\rangle \\
					0 & \text{otherwise} 
					\end{cases}
				\end{equation*}
		\end{itemize}
		\item $\Omega'=\Omega \cup \{o_F\}$, the observation spate
		\item $Obs':S'\to \Omega'$ where 
			\begin{equation*}
				Obs'(s)= \begin{cases}
				Obs(s') & \text{if } s=\langle s',n\rangle\\
				o_F & \text{if } s=s_F
				\end{cases}
			\end{equation*}
		\end{itemize}
\end{definition}


