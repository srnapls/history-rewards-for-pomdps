We study reward controllers (RCs), which emulate history-based reward functions specified for partially observable Markov decision processes (POMDPs). We show how to obtain these reward controllers from a number of sequences or regular expressions together with their respective rewards. We combine this reward controller together with the POMDP it is defined over to create an induced POMDP. This allows us to obtain a policy for obtaining the maximum expected reward for the original POMDP. 