We study how to obtain reward controllers (RCs) from history-based reward functions specified for partially observable Markov decision processes (POMDPs). A history-based reward function can be specified as a number of sequences or regular expressions together with their respective rewards. We create a product of the RC and the original POMDP to obtain an induced POMDP. Then, allowing for a maximum length of the sequence, we show how to obtain a policy for obtaining the maximum expected reward for the original POMDP.