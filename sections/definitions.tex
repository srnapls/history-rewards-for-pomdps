\section{Finite Automata}
\subsection{Deterministic Finite Automata}
Simple deterministic processes can be easily modeled with the help of a finite-state machine. Specifically, if we are interested in wether an input string should be accepted, we can use Deterministic Finite Automata.

\begin{definition}[DFA]
A deterministic finite automaton is a tuple $D=(Q,q_0,\Sigma,\delta,F)$ where 
\begin{itemize}
\item $Q$, the finite set of states;
\item $q_0$, the initial state;
\item $\Sigma$ the input alphabet;
\item $\delta: Q\times\Sigma\to Q$, the deterministic transition function;
\item $F\subseteq Q$, the set of final states.
\end{itemize}
\end{definition}


\subsection*{Example}
\begin{figure}[H]
	\centering
		\begin{tikzpicture}[node distance=3cm,on grid,auto,shorten >=1pt]
			\node[initial,state,accepting] (0) 				{$q_0$};
			\node[state] (1) [above right=of 0]		{$q_1$};
			\node[state] (2) [below right=of 0]		{$q_2$};
			\path[->] 	(0) edge 	node [below] {$a$}	(1)
							edge [bend right]   node [below] {$b$}	(2)
						(1) edge 	node [left] {$a$}	(2)
							edge [bend right]    node [above] {$b$}	(0)
						(2) edge 	node [above] {$a$}	(0)
							edge [bend right]    node [right] {$b$}	(1);
	\end{tikzpicture}
\caption{DFA over $\Sigma=\{a,b\}$ which accepts words if the number of $a$'s and $b$'s are equal modulo $3$.}
\end{figure}

Since we are interested in wether an input string should be accepted or not, we are specifically interested in how a DFA handles certain words and where a DFA will finish after reading a word. Since DFAs are deterministic, this can be easily described. 

\begin{definition}
We define $\delta^*:Q\times\Sigma^*\to Q$ where $\delta^*(q,w)$ denotes the state we end up after reading word $w$ starting from state $q$ as follows
\begin{equation*}
\delta^*(q,w)=\begin{cases}
	q &\text{if } w=\epsilon \\
	\delta^*(\delta(q,a_1),a_2\dots a_n) & \text{if } w=a_1a_2\dots a_n
	\end{cases}
\end{equation*}
\label{d:delta_star}
\end{definition}

\begin{definition}
We say the language accepted by a DFA $D=(Q,q_0,\Sigma,\delta,F)$ consists of all the words that start in the begin state and finish in any final state. Thus $L(D)=\{w\in\Sigma^*\mid \delta^*(q_0,w)\in F\}$.
\label{d:accepted_language}
\end{definition}


\subsection{Moore machine}
\towrite{why what how}
Based on the definition as presented in \cite{p:moore}.
\begin{definition}
	A Moore machine is a tuple $(Q,q_0,\Sigma,O,\delta,\sigma)$ where
	\begin{itemize}
		\item $Q$, the finite set of states;
		\item $q_0\in Q$, the initial state;
		\item $\Sigma$, the finite set of input characters - the input alphabet;
		\item $O$, the finite set of output characters - the output alphabet;
		\item $\delta: Q\times \Sigma\to Q$, the input transition function, and;
		\item $\sigma : Q\to O$, the output transition function.
	\end{itemize}
\end{definition}
\subsection*{Example}
\towrite{example moore machine -- traditional sense}

\section{Markov Processes}
Some processes are not deterministic, but instead rely on probabilities.
\towrite{specifically markovian -- only The probability of the next event is only dependent on the current event. } 

\towrite{introduction..}


\subsection{Markov decision processes}

\towrite{introduction to MDP}
\begin{definition}[MDP]
	A Markov decision process is a tuple $M=(S,s_I,A,T)$ where 
	\begin{itemize}
		\item $S$, the finite set of states;
		\item $s_I\in S$, the initial state;
		\item $A$, the finite set of actions;
		\item $T:S\times A\to \Pi(S)$, the probabilistic transition function.
	\end{itemize}
\end{definition}

Note that given $s\in S,a\in A$, we assign a probability distribution over $S$ through $T(s,a)$. To obtain the probability of ending up in a certain state $s'$ when starting in state $s$ and performing action $a$, we simply calculate $T(s,a,s')$ which we obtain through $T(s,a)(s')$.\\

The \textit{available actions} for a state $s$ are given by $A(s)=\{a\in A\mid \exists s'\in S: T(s,a,s')>0\}$. We can give the \textit{possible successors} of state $s$ in a similar matter through $Succ(s)=\{s\in S\mid\exists a\in A : T(s,a,s')>0\}$.

A finite \textit{trajectory} or \textit{run} $\pi$ of an MDP is realization of the stochastic process performed by the MDP denoted by the finite sequence $s_1 a_1 s_2 a_2\dots s_{n-1} a_{n-1} s_n \in (S\times A)^*\times S$. To obtain the last state of a trajectory we can use the following \[last(\pi)=last(s_1 a_1 s_2 a_2\dots s_{n-1} a_{n-1} s_n)=s_n\].

\subsubsection*{Rewards}
We can extend MDPs with a \textit{reward function} $R$ which assign a reward - usually in $\mathbb{R}$ for taking a certain action $a$ in a state $s$. 

\towrite{Intuitive explanation for reward function - including cost function, including a real-world example}

Let us look at \textit{Markovian reward functions}, which can determine a reward based on the current state, action and obtained state, independent of its history. The most conventional notation is $R:S\times A\to \mathbb{R}$, where we consider the current state and the taken action. Another possible definition is $R:S\times A\times S\to\mathbb{R}$, where  in $R(s,a,s')$ we consider the specific transition from $s$ to $s'$ by using action $a$, or $R:S\to\mathbb{R}$ where in $R(s)$ we only consider the visited state $s$. 

\towrite{Real life example of reward function with history}

A reward function which is dependent of its history is called a \textit{Non-Markovian reward function}. There are a number of different reward functions possible
\begin{itemize}
	\item $R:S^*\to\mathbb{R}$ - which only looks at the finite states visited, or;
	\item $R:(S\times A)^*\to\mathbb{R}$ - which looks at the finite (sub)trajectory without the last state, or;
	\item $R:(S\times A)^*\times S\to \mathbb{R}$ - which looks at the finite (sub)trajectory.
\end{itemize}

The reward function we will be using is the Non-Markovian reward function which looks at trajectories of specific length $k$, namely $R_k:(S\times A)^k\to\mathbb{R}$. 

\towrite{Increasing k creates increased reward}


\subsubsection*{Policy}
As stated above, we use reward functions over a MDP to usually argue over an optimized expected reward. After retrieving such an optimum, the question remains on how to actually obtain this value. We wish to know what strategy to apply path to take to obtain this value. For this we use strategies, or often called policies. 

\begin{definition}
	A policy for a MDP $M$ is a function $\sigma:(S\times A)^*\times S \to \Pi(A)$, which maps a trajectory $\pi$ to a probability distribution over all actions. 
\end{definition}

We call a policy \textit{memoryless} if the function only considers $last(\pi)$ in deciding the actions. 

\subsection{Partial observability}
\towrite{Introduce pomdp}

\begin{definition}[POMDP]
	A partially observable Markov decision process (POMDP) is a tuple $\mathcal{M}=(M, \Omega, O)$ where 
	\begin{itemize}
		\item $M=(S,s_I,A,T)$, the hidden MDP;
		\item $\Omega$, the finite set of observations;
		\item $O:S\to \Omega$, the observation function. 
	\end{itemize}
\end{definition}

Let $O^{-1}:\Omega\to 2^S$ be the inverse function of the observation function - $O^{-1}(o)=\{s\in \mid O(s)=o\}$ - in which we simply obtain all states in $S$ that have observation $o$.
Without loss of generality we assume that states with the same observations have the same set of available actions, thus $O(s_1)=O(s_2)\Rightarrow A(s_1)=A(s_2)$.

Since the actual states in a trajectory of the hidden MDP are not visible to the observes, we argue about an \textit{observed trajectory} of the POMDP $\mathcal{M}$. This is not consist of a sequence of states and actions, but instead a sequence of observations are actions, thus an element of $(\Omega\times A)^*\times \Omega$. The set of all possible finite observed trajectories of will be denoted as $ObsSeq^{\mathcal{M}}$.

We can argue about the observed trajectory through the observation function, which will be extended over trajectories, like so
\[O(\pi)=O(s_1 a_1 s_2 a_2\dots s_{n-1} a_{n-1} s_n) = O(s_1) a_1 O(s_2) a_2\dots O(s_{n-1}) a_{n-1} O(s_n)\]

\towrite{pomdp with reward}

\subsubsection*{Policy}
\begin{definition}
	An observation-based strategy of a POMDP $\mathcal{M}$ is a function $\sigma:ObsSeq^{\mathcal{M}}\to\Pi(A)$ such that $supp(\sigma(O(\pi)))\subseteq A (last(\pi))\ \forall \pi\in(S\times A)^*\times S$.
\end{definition}

\subsection{Belief MDP}

\begin{definition}[Belief state]
A belief state $b:S\to [0,1]$ is a probability distribution over $S$. For every state $s$, $b(s)$ denotes the probability of currently being in state $s$.
\end{definition}

\towrite{introduction to beflief update}
\begin{definition}[Belief update]
Given the current belief state $b$, then after performing action $a\in A$ and then observing observation $o$, we update the belief state. The updated belief state $b^{a,o}$ can be calculated as
\[b^{a,o}(s')= \frac{\text{Pr}(o\mid s',a)}{\text{Pr}(o\mid a,b)}\sum\limits_{s\in S}T(s,a,s')b(s)\]
\end{definition}
\towrite{connection to belief mdp}

\begin{definition}[Belief MDP]
For a POMDP $\mathcal{M}=(M,\Omega,O)$ where $M=(S,s_I,A,T)$ as defined above, the associated belief MDP is a tuple $(B,A,\tau,\rho)$ where 
\begin{itemize}
\item $B=\Pi(S)$, the set of belief states;
\item $A$, the set of actions;
\item $\tau:B\times A\times B$, the transition function where 
\[\tau(b,a,b')= \text{Pr}(b'\mid a,b)=\sum\limits_{o\in\Omega}\text{Pr}(b'\mid a,b,o)\cdot \text{Pr}(o\mid a,b)\]
\end{itemize}
\end{definition}

\subsubsection*{Reward}
If the POMDP is extended with a reward function $R$, the belief MDP will obtain a reward function $\rho$. If $R:S\times A\to \mathbb{R}$, then $\rho:B\times A\to \sum\limits_{s\in S}b(s)R(s,a)$. 

