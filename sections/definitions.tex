\section{Finite Automata}
\towrite{introduction to dfa}

\begin{definition}[DFA]
A Deterministic Finite Automata is a tuple $D=(Q,q_0,\Sigma,\delta,F)$ where 
\begin{itemize}
\item $Q$, the finite set of states;
\item $q_0$, the initial state;
\item $\Sigma$ the input alphabet;
\item $\delta: Q\times\Sigma\to Q$, the deterministic transition function;
\item $F\subseteq Q$, the set of final states.
\end{itemize}
\end{definition}

\towrite{introduction to lambda star}
\begin{definition}
We define $\delta^*:Q\times\Sigma^*\to Q$ where $\delta^*(q,w)$ denotes the state we end up after reading word $w$ starting from state $q$ as follows
\begin{equation*}
\delta^*(q,w)=\begin{cases}
	q &\text{if } w=\epsilon \\
	\delta^*(\delta(q,a_1),a_2\dots a_n) & \text{if } w=a_1a_2\dots a_n
	\end{cases}
\end{equation*}
\label{d:delta_star}
\end{definition}

\towrite{introduction to accepted words}

\begin{definition}
We say the language accepted by a DFA $D=(Q,q_0,\Sigma,\delta,F)$ consists of all the words that start in the begin state and finish in any final state. Thus $L(D)=\{w\in\Sigma^*\mid \delta^*(q_0,w)\in F\}$.
\label{d:accepted_language}
\end{definition}



\section{Markov decision processes}

\towrite{introduction to MDP}
\begin{definition}[MDP]
	A Markov decision process is a tuple $M=(S,s_I,A,T)$ where 
	\begin{itemize}
		\item $S$, the finite set of states;
		\item $s_I\in S$, the initial state;
		\item $A$, the finite set of actions;
		\item $T:S\times A\to \Pi(S)$, the probabilistic transition function.
	\end{itemize}
\end{definition}

Note that given $s\in S,a\in A$, we assign a probability distribution over $S$ through $T(s,a)$. To obtain the probability of ending up in a certain state $s'$ when starting in state $s$ and performing action $a$, we simply calculate $T(s,a,s')$ which we obtain through $T(s,a)(s')$.\\

The \textit{available actions} for a state $s$ are given by $A(s)=\{a\in A\mid \exists s'\in S: T(s,a,s')>0\}$. We can give the \textit{possible successors} of state $s$ in a similar matter through $Succ(s)=\{s\in S\mid\exists a\in A : T(s,a,s')>0\}$.

A finite \textit{trajectory} or \textit{run} $\pi$ of an MDP is realization of the stochastic process performed by the MDP denoted by the finite sequence $s_1 a_1 s_2 a_2\dots s_{n-1} a_{n-1} s_n \in (S\times A)^*\times S$. To obtain the last state of a trajectory we can use the following \[last(\pi)=last(s_1 a_1 s_2 a_2\dots s_{n-1} a_{n-1} s_n)=s_n\].

\subsection*{Reward function}
We can extend MDPs with a \textit{reward function} $R$ which assign a reward - usually in $\mathbb{R}$ for taking a certain action $a$ in a state $s$. 

\towrite{Intuitive explanation for reward function - including cost function, including a real-world example}

Let us look at \textit{Markovian reward functions}, which can determine a reward based on the current state, action and obtained state, independent of its history. The most conventional notation is $R:S\times A\to \mathbb{R}$, where we consider the current state and the taken action. Another possible definition is $R:S\times A\times S\to\mathbb{R}$, where  in $R(s,a,s')$ we consider the specific transition from $s$ to $s'$ by using action $a$, or $R:S\to\mathbb{R}$ where in $R(s)$ we only consider the visited state $s$. 

\towrite{Real life example of reward function with history}

A reward function which is dependent of its history is called a \textit{Non-Markovian reward function}. There are a number of different reward functions possible
\begin{itemize}
	\item $R:S^*\to\mathbb{R}$ - which only looks at the finite states visited, or;
	\item $R:(S\times A)^*\to\mathbb{R}$ - which looks at the finite (sub)trajectory without the last state, or;
	\item $R:(S\times A)^*\times S\to \mathbb{R}$ - which looks at the finite (sub)trajectory.
\end{itemize}

The reward function we will be using is the Non-Markovian reward function which looks at trajectories of specific length $k$, namely $R_k:(S\times A)^k\to\mathbb{R}$. 

\towrite{Increasing k creates increased reward}


\subsection*{Policy}
As stated above, we use reward functions over a MDP to usually argue over an optimized expected reward. After retrieving such an optimum, the question remains on how to actually obtain this value. We wish to know what strategy to apply path to take to obtain this value. For this we use strategies, or often called policies. 

\begin{definition}
	A policy for a MDP $M$ is a function $\sigma:(S\times A)^*\times S \to \Pi(A)$, which maps a trajectory $\pi$ to a probability distribution over all actions. 
\end{definition}

We call a policy \textit{memoryless} if the function only considers $last(\pi)$ in deciding the actions. 

\towrite{induced markov chain for removing non-determinism}

\section{Partial observability}
\towrite{Introduce pomdp}

\begin{definition}[POMDP]
	A partially observable Markov decision process (POMDP) is a tuple $\mathcal{M}=(M, \Omega, O)$ where 
	\begin{itemize}
		\item $M=(S,s_I,A,T)$, the hidden MDP;
		\item $\Omega$, the finite set of observations;
		\item $O:S\to \Omega$, the observation function. %or use O: S x A -> \Omega ?
	\end{itemize}
\end{definition}

Let $O^{-1}:\Omega\to 2^S$ be the inverse function of the observation function - $O^{-1}(o)=\{s\in \mid O(s)=o\}$ - in which we simply obtain all states in $S$ that have observation $o$.
Without loss of generality we assume that states with the same observations have the same set of available actions, thus $O(s_1)=O(s_2)\Rightarrow A(s_1)=A(s_2)$.

Since the actual states in a trajectory of the hidden MDP are not visible to the observes, we argue about an \textit{observed trajectory} of the POMDP $\mathcal{M}$. This is not consist of a sequence of states and actions, but instead a sequence of observations are actions, thus an element of $(\Omega\times A)^*\times \Omega$. The set of all possible finite observed trajectories of will be denoted as $ObsSeq^{\mathcal{M}}$.

We can argue about the observed trajectory through the observation function, which will be extended over trajectories, like so
\[O(\pi)=O(s_1 a_1 s_2 a_2\dots s_{n-1} a_{n-1} s_n) = O(s_1) a_1 O(s_2) a_2\dots O(s_{n-1}) a_{n-1} O(s_n)\]

\subsection*{Policy}
\begin{definition}
	An observation-based strategy of a POMDP $\mathcal{M}$ is a function $\sigma:ObsSeq^{\mathcal{M}}\to\Pi(A)$ such that $supp(\sigma(O(\pi)))\subseteq A (last(\pi))\ \forall \pi\in(S\times A)^*\times S$.
\end{definition}

\section{Belief MDP}



\section{Moore machine}
Based on the definition as presented in \cite{p:moore}.
\begin{definition}
	A Mealy machine is a tuple $(Q,q_0,\Sigma,O,\delta,\sigma)$ where
	\begin{itemize}
		\item $Q$, the finite set of states;
		\item $q_0\in Q$, the initial state;
		\item $\Sigma$, the finite set of input characters - the input alphabet;
		\item $O$, the finite set of output characters - the output alphabet;
		\item $\delta: Q\times \Sigma\to Q$, the input transition function, and;
		\item $\sigma : Q\to O$, the output transition function.
	\end{itemize}
\end{definition}
\subsection*{Example}
\towrite{example moore machine -- traditional sense}