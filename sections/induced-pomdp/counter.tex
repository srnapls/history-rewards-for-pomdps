\label{s:induced_limit}
However, when we try to calculate the probability for which we end up in $s_F$ ($P[ \texttt{F}\:s=s_F]$), we notice that this probability is not equal to 1, because there is no absolutele certainty that \texttt{end} will ever be performed and thus no certainty that we end up in $s_F$. Luckily, we can enforce that the model only allows observation sequences up to a certain natural number $T$, such that for every observation sequence $o_1 o_2 \dots o_n$ we know that $n\leq T$. 

\begin{definition}
We can extend the underlying MDP $M=(S\cup\{s_F\}, s_I, A, T_M)$ of the extended POMDP $\widetilde{M}=(M,\Omega,O)$ with some given counter $T$, creating a limited underlying MDP $(S',s'_I,A,T_M')$ where
\begin{itemize}
	\item $S' = S\times \{0,\dots, T\}$
	\item $s'_I = \langle s_I,0\rangle$
	\item $T_M':S'\times A \to \Pi(S')$ where 
	\[T_M'(\langle s_1,t_1\rangle,a,\langle s_2,t_2\rangle) = \begin{cases}
	T_M(s_1,a,s_2) & \text{ if } t_2 = t_1 + 1 \text{ and } t_2 \neq T\\
	1 & \text{ if } t_2 = T \text{ and } s_2 = s_F \\
	0 & \text{ otherwise }
	\end{cases}\]
\end{itemize}
\label{d:limited_pomdp}

The reward function $R:S'\times A\times S' \to \mathbb{R}$ will only look at the states of $S$ and will be transformed into
\[R(\langle s_1, t_1\rangle, a, \langle s_2, t_2\rangle) = R(s_1,a,s_2)\]
\end{definition}

In other words, we add a simple counter that keeps track of the number of actions allowed. At any point we are allowed to go to the final state, but when the counter reaches $T$, we force the model to enter the final state. This method enforces the process to always finish in the final state. \\

Now since that we know that $P[ \texttt{F}\:s=s_F]=1$, we can calculate the expected maximum reward for when we enter that state. In other words, we can use model checking tools like \texttt{PRISM} to calculate $R_{max}[\texttt{F}\:s=s_F]$.


\towrite{size and complexity}