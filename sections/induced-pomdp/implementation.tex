\towrite{the python part, where we simple combine the information of the reward controller together with the pomdp (without reward) together to create a new pomdp}

remove the end action from prism and code it into the pomdp in python. check if this is possible. 
\towrite{
the transformation to prism where we then add the extra \texttt{end} actions with the last state added.\\
- limit to $T$, which needs to be passed along \\
- for the end action, we only need to observation 
}