We will now combine the reward controller $\mathcal{F}$ with the given related PODMP to obtain an induced POMDP where we map the memory into the system. This ensures that we don't have to keep the observation sequence in memory. 

\begin{definition}
	\label{def:induced-pomdp}
	The induced POMDP for reward controller $\mathcal{F}=(N, n_I, \Omega, \mathcal{R}, \delta, \sigma)$ on a POMDP $\mathcal{M}=(M,\Omega,O)$ where $M=(S,s_I,A,T_{M})$ is a tuple $\mathcal{M_\mathcal{F}}=(M_\mathcal{F},\Omega,O_\mathcal{F})$ where 
	\begin{itemize}
		\item $M_\mathcal{F} = (S_\mathcal{F},s_{I,\mathcal{F}}, A, T_{M_\mathcal{F}})$, the hidden MDP where:
		\begin{itemize}
			\item $S_\mathcal{F}=S\times N$, the finite set of states;
			\item $s_{I,\mathcal{F}} = \langle s_I, \delta(n_I, O(s_I))\rangle$, the initial state;
			\item $T_{M_\mathcal{F}}:S_\mathcal{F}\times A \to \Pi(S_\mathcal{F})$, the probabilistic transition function defined as:
				\begin{align*}				
					T_{M_\mathcal{F}}(\langle s,n\rangle,a,\langle s',n'\rangle) &= \begin{cases}
						T_M(s,a,s') & \text{if } \delta(n,O(s')=n') \\
						0 & \text{otherwise}
					\end{cases}
				\end{align*}
		\end{itemize}
		\item $O_\mathcal{F}:S_\mathcal{F}\to \Omega$, the observation function where 
		\[O_\mathcal{F}(\langle s,n \rangle) = O(s)\]
		\end{itemize}
\end{definition}

Now we can extend this induced POMDP $\mathcal{M}_\mathcal{F}$ as presented in \defref{d:extended-pomdp}, yielding $\widetilde{\mathcal{M}_\mathcal{F}}$, with the adjusted reward function in which we obtain the related reward when we enter the final state through action \texttt{end}.

\begin{definition}
	\label{def:reward-fuction-extended-induced-pomdp}
	The reward function $\mathcal{R}:S_\mathcal{F}'\times A\to \mathbb{R}$ for the extended induced POMDP $\widetilde{\mathcal{M}_{\mathcal{F}}}$ is defined as follows:
	\[\mathcal{R}(s,a) = \begin{cases}
		\sigma(n) & \text{if } a=\texttt{end} \text{ and } s=\langle s'',n\rangle \\
		0 & \text{otherwise} 
		\end{cases}
	\]
\end{definition}

Note that for the POMDP $\mathcal{M}$ we could only calculate the reward after we were done with the process. However, for the newly obtained POMDP $\mathcal{M}_{\mathcal{F}}$ we obtain the reward as the process continues, since it is now dependent only on the state and action.