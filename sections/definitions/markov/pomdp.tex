\subsection{Partial observability}
Having full observability over a system makes it simple to calculate the optimal expected reward and there has been numerous research finding the optimal expected reward or finding a policy that ensures a certain wanted property. However, when modeling a lot of real world issues, we unfortunately do not have all the information readily available to us.

Take for example a machine that breaks down over a period and needs repairs\cite{p:maintenance}. We know that certain parts in this machine deteriorate at different rates, but we do not know the exact state of each part in this machine. However, we can \textit{observe} the entire system, which only providus us with partial knowledge of the machine. We can model these types of systems with the help of partially observable MDPs.

\begin{definition}[POMDP]
	A partially observable Markov decision process (POMDP) is a tuple $\mathcal{M}=(M, \Omega, O)$ where 
	\begin{itemize}
		\item $M=(S,s_I,A,T)$, the hidden MDP;
		\item $\Omega$, the finite set of observations;
		\item $O:S\to \Omega$, the observation function. 
	\end{itemize}
\end{definition}

Let $O^{-1}:\Omega\to 2^S$ be the inverse function of the observation function - $O^{-1}(o)=\{s\in \mid O(s)=o\}$ - in which we simply obtain all states in $S$ that have observation $o$.
Without loss of generality we assume that states with the same observations have the same set of available actions, thus $O(s_1)=O(s_2)\Rightarrow A(s_1)=A(s_2)$.

Since the actual states in a trajectory of the hidden MDP are not visible to the observes, we argue about an \textit{observed trajectory} of the POMDP $\mathcal{M}$. This is not consist of a sequence of states and actions, but instead a sequence of observations are actions, thus an element of $(\Omega A)^*\Omega$. The set of all possible finite observed trajectories of POMDP $\mathcal{M}$ will be denoted as $ObsSeq^{\mathcal{M}}$.

We can argue about the observed trajectory through the observation function, which will be extended over trajectories, like so
\[O(s_1 a_1 s_2 a_2\dots s_{n-1} a_{n-1} s_n) = O(s_1) a_1 O(s_2) a_2\dots O(s_{n-1}) a_{n-1} O(s_n)\]

\subsubsection*{Example}
\begin{figure}[H]
	\centering
		\begin{tikzpicture}[node distance=1cm,on grid,auto]
			\node[initial,state] (0) 			{$s_0$};
			\node[dot]  (0a) [below=of 0] {};
			\node[dot]  (0b) [right=of 0] {};
			\node[state] (1) [right= 7cm of 0]		{$s_1$};
			\node[dot] (1a) [below=of 1] {};
			\node[state,fill=red] (2) [below right= 5 cm of 0]		{$s_2$};
			\node[dot] (2c) [above=of 2] {};
			\path[-,blue]    (0) edge [] node[right] {$a$} (0a)
                            edge [] node[above] {$b$} (0b)
                        (1) edge [] node[left]  {$a$} (1a)
                        (2) edge [] node[right] {$c$} (2c);
			\path[->] (0a)  edge[bend left] 	node [left] {$0.5$}  (0)
			                edge[bend right] node [below] {$0.5$}  (2)
			          (0b)  edge[]              node [above] {$1$} (1)
			          (1a)  edge[bend right]     node [right] {$0.25$} (1)
			                edge[bend left]    node [left]  {$0.75$} (2)
			          (2c)  edge[bend right]     node [left] {$0.8$}  (2)
			          	 	edge[]		node [left] {$0.1$} (0)
			                edge[]    node [right] {$0.1$}  (1);
	\end{tikzpicture}
\caption{Example POMDP where $\Omega=\{\texttt{white},\texttt{red}\}$}
\end{figure}

\subsubsection*{Rewards}
Just like for MDPs, we can extend POMDPs with a reward function. These can be function over the hidden MDP, and thus the reward function will remain an extension of the MDP. Or, the function can be based on the observations of the states instead of the states themselves. So instead of $R:S\times A\times S\to\mathbb{R}$, we can have that $R:\Omega\times A\times\Omega\to\mathbb{R}$. This way we only base the reward function on the observation of the current and future state together with the action taken. There is also a possibility for history-based reward function based on the observations, e.g. $R:\Omega^*\to\mathbb{R}$.

\subsubsection*{Policy}
Defining a policy over a POMDP is a trickier, since we only obtain the observation of a state and not all the information. So now we have to make a decision over what action to take, given only the observations. This provides us with observation-based trategies. 

\begin{definition}
	An observation-based strategy of a POMDP $\mathcal{M}$ is a function $\pi:ObsSeq^{\mathcal{M}}\to\Pi(A)$ such that 
	\[supp(\pi(O(s_1 a_1 \dots s_{n-1} a_{n-1} s_n)))\subseteq A (last(s_1 a_1 \dots s_{n-1} a_{n-1} s_n))\] for all $s_1 a_1 \dots s_{n-1} a_{n-1} s_n\in(S\times A)^*\times S$.
\end{definition}
