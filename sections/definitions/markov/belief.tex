\subsection{Belief MDP}

\begin{definition}[Belief state]
A belief state $b:S\to [0,1]$ is a probability distribution over $S$. For every state $s$, $b(s)$ denotes the probability of currently being in state $s$.
\end{definition}

\towrite{introduction to beflief update}
\begin{definition}[Belief update]
Given the current belief state $b$, then after performing action $a\in A$ and then observing observation $o$, we update the belief state. The updated belief state $b^{a,o}$ can be calculated as
\[b^{a,o}(s')= \frac{\text{Pr}(o\mid s',a)}{\text{Pr}(o\mid a,b)}\sum\limits_{s\in S}T(s,a,s')b(s)\]
\end{definition}
\towrite{connection to belief mdp}

\begin{definition}[Belief MDP]
For a POMDP $\mathcal{M}=(M,\Omega,O)$ where $M=(S,s_I,A,T)$ as defined above, the associated belief MDP is a tuple $(B,A,\tau,\rho)$ where 
\begin{itemize}
\item $B=\Pi(S)$, the set of belief states;
\item $A$, the set of actions;
\item $\tau:B\times A\to \Pi(B)$, the transition function where 
\[\tau(b,a,b')= \text{Pr}(b'\mid a,b)=\sum\limits_{o\in\Omega}\text{Pr}(b'\mid a,b,o)\cdot \text{Pr}(o\mid a,b)\]
\end{itemize}
\end{definition}

\towrite{note that belief mdp are continuous time and not discrete}

\subsubsection*{Reward}
If the POMDP is extended with a reward function $R$, the belief MDP will obtain a reward function $\rho$. If $R:S\times A\to \mathbb{R}$, then $\rho:B\times A\to \mathbb{R}$ where $\rho(b,a) = \sum\limits_{s\in S}b(s)R(s,a)$. 

\subsubsection*{Solving for optimal reward}
$Pr(o\mid a,b) = \sum\limits_{s\in S} \sum\limits_{s'\in O^{-1}(o)}T(s,a,s')b(s)$

\begin{flalign*}
V_0(b) & = 0 \\
V_n(b) & = \max\limits_{a\in A} \big[\rho(b,a) + \gamma \sum\limits_{o \in \Omega} Pro(o\mid a,b) V_{n-1}(b^{a,o})\big]
\end{flalign*}