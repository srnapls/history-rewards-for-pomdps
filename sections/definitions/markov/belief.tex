\subsection{Belief MDP}
As stated in the previous section, POMDPs only have partially observable, i.e. we do not have full knowledge over the system. More specifically, we do not have full information available over which state we are in, only the observation. Since we cannot argue about which exact state we are in, we argue about which state we \textit{belief} we are in. 

\begin{definition}[Belief state]
A belief state $b:S\to [0,1]$ is a probability distribution over $S$. For every state $s$, $b(s)$ denotes the probability of currently being in state $s$.
\end{definition}

When we perform an action after observing a certain observation and then observe (a different) observation, we need to update our belief about the state we are in.  After the transition, we have a transitioned into a new belief state. This update is called a \textit{belief update}. 

\begin{definition}[Belief update]
Given the current belief state $b$, then after performing action $a\in A$ and then observing observation $o\in\Omega$, we update the belief state. The updated belief state $b^{a,o}$ can be calculated as
\[b^{a,o}(s')= \frac{\text{Pr}(o\mid s',a)}{\text{Pr}(o\mid a,b)}\sum\limits_{s\in S}T(s,a,s')b(s)\]
\end{definition}

When working with these probabilistic processes, it is preferable to have full observability into the system to accurately calculate certain wanted properties. To remove this partial observability, we can transform a POMDP into a fully observable \textit{belief MDP}. 

\begin{definition}[Belief MDP]
For a POMDP $\mathcal{M}=(M,\Omega,O)$ where $M=(S,s_I,A,T)$ as defined above, the associated belief MDP is a tuple $(B,A,\tau,\rho)$ where 
\begin{itemize}
\item $B=\Pi(S)$, the set of belief states;
\item $A$, the same set of actions;
\item $\tau:B\times A\to \Pi(B)$, the transition function where 
\[\tau(b,a,b')= \text{Pr}(b'\mid a,b)=\sum\limits_{o\in\Omega}\text{Pr}(b'\mid a,b,o)\cdot \text{Pr}(o\mid a,b)\]
\end{itemize}
\end{definition}

Note that the belief MDP needs to have an initial belief state. This initial state distribution $b_0$ is sometimes also an extension of the original POMDP. If only the actual initial state is given as $s_I$, then we can simply calculate $b_0$ as $b_0(s_I)=1$ and $b_0(s)=0$ for all $s\in S\setminus\{s_I\}$. We also have to acknowledge that working with these belief representations of a given POMDP may yield a continuous, instead of a discrete, model.

\subsubsection*{Reward}
If the POMDP is extended with a reward function $R$, the belief MDP will obtain a reward function $\rho$. If $R:S\times A\to \mathbb{R}$, then $\rho:B\times A\to \mathbb{R}$ where $\rho(b,a) = \sum\limits_{s\in S}b(s)R(s,a)$. 

\subsubsection*{Solving for optimal reward}

Now we can also compute the value function. Let $Pr(o\mid a,b) = \sum\limits_{s\in S} \sum\limits_{s'\in O^{-1}(o)}T(s,a,s')b(s)$ be in the following function.

\begin{flalign*}
V_0(b) & = 0 \\
V_n(b) & = \max\limits_{a\in A} \big[\rho(b,a) + \gamma \sum\limits_{o \in \Omega} Pr(o\mid a,b) V_{n-1}(b^{a,o})\big]
\end{flalign*}