\subsection*{Markov decision processes}
While we can see Markov chains as stochastic processes without outside influence, we can also take a look at these processes where we allow outside influence. This is done by extending the Markov chain with a set of actions, allowing for this influence. 
\begin{definition}[MDP]
	A Markov decision process is a tuple $M=(S,s_I,A,T)$ where 
	\begin{itemize}
		\item $S$, the finite set of states;
		\item $s_I\in S$, the initial state;
		\item $A$, the finite set of actions;
		\item $T:S\times A\to \Pi(S)$, the probabilistic transition function.
	\end{itemize}
\end{definition}

Note that given $s\in S,a\in A$, we assign a probability distribution over $S$ through $T(s,a)$. To obtain the probability of ending up in a certain state $s'$ when starting in state $s$ and performing action $a$, we simply calculate $T(s,a,s')$ which we obtain through $T(s,a)(s')$.\\

The \textit{available actions} for a state $s$ are given by $A(s)=\{\,a\in A\mid \exists s'\in S: T(s,a,s')>0\}$. We can give the \textit{possible successors} of state $s$ in a similar matter through $Succ(s)=\{s\in S\mid\exists a\in A : T(s,a,s')>0\}$.

A finite \textit{trajectory} or \textit{run} of a MDP is realization of the stochastic process performed by the MDP denoted by the finite sequence $s_1 a_1 s_2 a_2\dots s_{n-1} a_{n-1} s_n \in (S\times A)^*\times S$. To obtain the last state of a trajectory we can use the following \[last(s_1 a_1 s_2 a_2\dots s_{n-1} a_{n-1} s_n)=s_n\]

\subsubsection*{Example}
\begin{figure}[H]
	\centering
		\begin{tikzpicture}[node distance=1cm,on grid,auto]
			\node[initial,state] (0) 			{$s_0$};
			\node[dot]  (0a) [below=of 0] {};
			\node[dot]  (0b) [right=of 0] {};
			\node[state] (1) [right= 7cm of 0]		{$s_1$};
			\node[dot] (1a) [below=of 1] {};
			\node[state] (2) [below right= 5 cm of 0]		{$s_2$};
			\node[dot] (2c) [above=of 2] {};
			\path[-,blue]    (0) edge [] node[right] {$a$} (0a)
                            edge [] node[above] {$b$} (0b)
                        (1) edge [] node[left]  {$a$} (1a)
                        (2) edge [] node[right] {$c$} (2c);
			\path[->] (0a)  edge[bend left] 	node [left] {$0.5$}  (0)
			                edge[bend right] node [below] {$0.5$}  (2)
			          (0b)  edge[]              node [above] {$1$} (1)
			          (1a)  edge[bend right]     node [right] {$0.25$} (1)
			                edge[bend left]    node [left]  {$0.75$} (2)
			          (2c)  edge[bend right]     node [left] {$0.8$}  (2)
			          	 	edge[]		node [left] {$0.1$} (0)
			                edge[]    node [right] {$0.1$}  (1);
	\end{tikzpicture}
\caption{MDP}
\end{figure}

\subsubsection*{Rewards}
We can extend MDPs with a \textit{reward function} $R$ which assign a reward for taking some action in state. Let us first look at simple reward functions which can determine a reward based on the current state, action and obtained state, independent of its history. The most conventional notation for this simple reward function is $R:S\times A\to \mathbb{R}$, where we consider the current state and the taken action. Another possible definition is $R:S\times A\times S\to\mathbb{R}$, where  in $R(s,a,s')$ we consider the specific transition from $s$ to $s'$ by using action $a$, or $R:S\to\mathbb{R}$ where in $R(s)$ we only consider the visited state $s$. 

When modeling complex systems drawn from real world problems, we often encounter that obtaining a certain reward is not only dependent on the current events but also on the states (and actions) that were seen previously. These history-based reward functions are just as versatile as simple reward functions. A few examples are
\begin{itemize}
	\item $R:S^*\to\mathbb{R}$ - which only looks at the finite states visited, or;
	\item $R:(S A)^*\to\mathbb{R}$ - which looks at the finite (sub)trajectory without the last obtained state, or;
	\item $R:(S A)^* S\to \mathbb{R}$ - which looks at the finite (sub)trajectory.
\end{itemize}

\subsubsection*{Policy}
As stated above, we can extend MDPs with reward functions. Now when modeling a system, we usually want to obtain the expected maximum reward (or minimize the costs involved). However, just obtaining this reward is not enough without knowing how to obtain this. We wish to know what strategy we need to apply to obtain this optimal value. For this we use strategies, also known as policies. 

\begin{definition}[Policy]
	A policy for a MDP $M$ is a function $\pi:(S\times A)^*\times S \to \Pi(A)$, which maps a trajectory to a probability distribution over all actions. 
\end{definition}

We call a policy \textit{memoryless} if the function only considers the last state in deciding the actions. Note that we write $\pi(s,a)$ for $\pi(s)(a)$, which gives us the probability of performing action $a$ given the state $s$. We can apply these types of policies to a MDP to remove the non-determinism, resulting in an induced Markov chain.

\begin{definition}
Given a MDP $M=(S_M,s_I,A,T_M)$ and a memoryless policy $\pi:S\to\Pi(A)$, we obtain the induced Markov chain $M^\pi=(S,T)$ where we define the probabilistic transition function as follows \[ T(s,s') = \sum\limits_{a\in A} \pi(s,a) T(s,a,s')\]
\end{definition}

\subsubsection*{Solving for optimal reward}
We introduce a discounting factor $\gamma\in[0,1]$. This factor determines how interesting the immediate reward is. The closer $\gamma$ is to zero, we are mainly interested in the immedate rewards. However, the closer $\gamma$ is to one, we are more interested in all the future rewards. 

Let $\rho_t$ be the expected immediate reward obtained at a moment $t$ in the timeline used. The expected cumulative reward for simple reward functions with an undefined horizon starting in state $s$s, given a policy $pi$, is defined as

\[
	J^\pi(s)=E \big[\sum_{t=0}^{\infty}\gamma \rho_t\mid s,\pi\big]
\]

With this information, we can obtain the optimal policy through
\begin{equation}
\pi^* = \argmax_{\pi\in \Pi(A)^S} J^\pi(s_I)
\end{equation}

We can solve the problem of how to obtain a policy for the optimal expected reward recursively, due to Bellman's principle of optimality\cite{p:bellman}.
\begin{flalign*}
V_0(s) & = 0 \\
V_n(s) &= \max\limits_{a\in A} \big[R(s,a) + \gamma\sum\limits_{s'\in S}T(s,a,s')V_{n-1}(s')\big]
\end{flalign*}
If we let $H$ be the possible infinite horizon of the problem, the optimal value can be concluded from $J^{\pi^*}(s)=V_{n=H}(s)$.

We can also apply a policy to the value function, to obtain the expected reward using policy $\pi$ as follows
\begin{flalign*}
V_{\pi,0}(s) & = 0 \\
V_{\pi,n}(s) &= \sum\limits_{a\in A}\pi(s,a) \big( R(s,a) + \gamma\sum\limits_{s'\in S}T(s,a,s')V_{\pi, n-1}(s')\big)
\end{flalign*}